<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Linux wget for downloading</title>
      <link href="/2019/07/02/Linux-wget-for-downloading/"/>
      <url>/2019/07/02/Linux-wget-for-downloading/</url>
      
        <content type="html"><![CDATA[<p>wget for downloading files in Terminal.</p><p>When I was doing my summer project, I should download the datasets into the server. To achieve this, I used the <code>wget</code> and records the infor of this command by this chance.</p><h2 id="Introduction-of-wget"><a href="#Introduction-of-wget" class="headerlink" title="Introduction of wget"></a>Introduction of wget</h2><p>GNU Wget is a free utility for non-interactive download of files from Web. It supports HTTP, HTTPS, and FTP protocols, as well as retrieval through HTTP proxies. </p><p>Non-interactive means that it can work in the background. </p><p>To invoke:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget [option]… [URL]…</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><ol><li>GNU Wget <a href="https://www.gnu.org/software/wget/manual/wget.html" target="_blank" rel="noopener">1.20 Manual</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Can Python Interprete itself?</title>
      <link href="/2019/07/02/Can-Python-Interprete-itself/"/>
      <url>/2019/07/02/Can-Python-Interprete-itself/</url>
      
        <content type="html"><![CDATA[<p>The interpreter of Python can be written with Python, such as the most famous one <a href="http://pypy.org/" target="_blank" rel="noopener">PyPy</a>.</p><p>There are several advantages using <code>PyPy</code>, like the speed, memory usage, compatibility and stackless. </p><p>In the tutorial of Allison Kaptur, the Python interpreter is a stack machine: it manipulates several stacks to perform its operations( as contrasted with a register machine, which writes to and reads from particular memory locations).</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><ol><li>Allison Kaptur <a href="[http://aosabook.org/en/500L/a-python-interpreter-written-in-python.html](http://aosabook.org/en/500L/a-python-interpreter-written-in-python.html">A Python Interpreter Written in Python</a>)</li><li></li></ol>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python Interpreter</title>
      <link href="/2019/07/02/Python-Interpreter/"/>
      <url>/2019/07/02/Python-Interpreter/</url>
      
        <content type="html"><![CDATA[<p>python解释器</p><p>编写python代码时，得到的是一个包含python代码的<code>.py</code>为拓展名的文本文件。要运行代码的时候，需要python解释器去执行<code>.py</code>文件。</p><p>事实上，存在有很多python解释器。</p><h2 id="CPython"><a href="#CPython" class="headerlink" title="CPython"></a>CPython</h2><p>从python官网下载并安装好python3.x之后，我们会直接获得一个官方版本的解释器CPython. 这个解释器是用C语言开发的，所以叫CPython。</p><p>CPython是使用最广的Python解释器。当<code>.py</code>执行的时候，解释器会先解释成cpython文件，进而编译执行。</p><h2 id="IPython"><a href="#IPython" class="headerlink" title="IPython"></a>IPython</h2><p>IPython是基于CPython之上的一个交互式解释器，即IPython只是在交互方式上有增强，除此之外，其执行Python代码的功能和CPython是一样的。</p><p>CPython用<code>&gt;&gt;&gt;</code> 作为提示符，而IPython用<code>In[no.]:</code>作为提示符。</p><h2 id="PyPy"><a href="#PyPy" class="headerlink" title="PyPy"></a>PyPy</h2><p>PyPy是另一个解释器，目标是执行速度。PyPy采用了JIT技术，对Python代码进行动态编译（不是解释），所以可以显著提高Python代码的执行速度。</p><p>绝大部份Python代码都可以在PyPy下运行，但是PyPy和CPython有一些是不同的，导致相同的Python代码在两种解释器下执行可能有不同的结果。使用前要先了解其不同点。</p><h2 id="Jython"><a href="#Jython" class="headerlink" title="Jython"></a>Jython</h2><p>Jython是运行在Java平台上的Python解释器，可以直接把Python代码编译成Java字节码运行。</p><h2 id="IronPython"><a href="#IronPython" class="headerlink" title="IronPython"></a>IronPython</h2><p>IronPython和Jython类似，只不过其是运行在微软.NET平台上的Python解释器，可以直接把Python代码编译成.NET的字节码。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>廖雪峰 <a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1016966024263840" target="_blank" rel="noopener">python解释器</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Web Crawler Get the User-Agent</title>
      <link href="/2019/06/13/Web-Crawler-Get-the-User-Agent/"/>
      <url>/2019/06/13/Web-Crawler-Get-the-User-Agent/</url>
      
        <content type="html"><![CDATA[<h2 id="How-to-Get-User-Agent-for-your-Crawler"><a href="#How-to-Get-User-Agent-for-your-Crawler" class="headerlink" title="How to Get User-Agent for your Crawler"></a>How to Get User-Agent for your Crawler</h2><p>There are several common ways to get the user-agent and use it to scrape the website.</p><ol><li><p><code>about:version</code> in your browser, and check the User-Agent.</p></li><li><p>Use the inspect tool within your browser. Network -&gt; refresh your page -&gt; find the current page -&gt; Headers -&gt; check the User-Agent</p></li><li><p>install the <code>fake_agent</code> in your computer and import it for using in Python</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install fake_useragent</span><br></pre></td></tr></table></figure><p>When using:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"><span class="comment"># import random</span></span><br><span class="line"></span><br><span class="line">fake_ua=UserAgent()</span><br><span class="line">headers=&#123;<span class="string">'User-Agent'</span>:fake_ua.random&#125;</span><br></pre></td></tr></table></figure></li></ol><ol><li>Google to find some User-Agent, sush as <a href="https://blog.csdn.net/bone_ace/article/details/52476016" target="_blank" rel="noopener">User-Agent 汇总</a></li></ol><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>CSDN blog: <a href="https://blog.csdn.net/python_neophyte/article/details/82491359" target="_blank" rel="noopener">爬虫之UserAgent的获得方法</a></li><li>fake_useragent <a href="https://pypi.org/project/fake-useragent/" target="_blank" rel="noopener">doc</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Web Crawler </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Web Crawler </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Error Met in First Crawler Demo</title>
      <link href="/2019/06/13/Error-Met-in-First-Crawler-Demo/"/>
      <url>/2019/06/13/Error-Met-in-First-Crawler-Demo/</url>
      
        <content type="html"><![CDATA[<p>When writing the crawler program, I met across several problems. This blog is to records the solution which I used in my program.</p><h2 id="Access-denied-when-using-url-directly"><a href="#Access-denied-when-using-url-directly" class="headerlink" title="Access denied when using url directly"></a>Access denied when using url directly</h2><p>Some websites add some features to protect them from being scraped. In this case, we should add the <strong>headers</strong>. </p><p>The fields in <code>headers</code>:</p><ul><li>User-agent: like <code>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36</code></li><li>Referer: like <code>https://www.google.com/</code></li></ul><p>(See the <strong>introduction</strong> of crawler: <a href="/2019/06/13/Web-Crawler-Basic/" title="Web Crawler Basic">Web Crawler Basic</a>)</p><h2 id="Max-retries-exceeded-with-url"><a href="#Max-retries-exceeded-with-url" class="headerlink" title="Max retries exceeded with url"></a>Max retries exceeded with url</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requests.exceptions.SSLError: HTTPSConnectionPool(host=&apos;www.mzitu.com&apos;, port=443): Max retries exceeded with url: /184325/8 (Caused by SSLError(SSLError(&quot;bad handshake: SysCallError(60, &apos;ETIMEDOUT&apos;)&quot;)))</span><br></pre></td></tr></table></figure><p>First time to solve this question:</p><ul><li>add <code>sleep(1)</code> in every iteration of downloading image<ul><li>It improve the problem and make the program stick longer time to do crawler work, but not solve it completely.</li><li>Be able to download around 300 images until failing.</li></ul></li><li>To improve the ability of anti-anti-spider, in the second version, I added two other functions:<ul><li>Randomly sleep some time when accessing the images</li><li>Randomly choose the User-Agent using <code>fake_useragent</code> Python module.</li><li>The performance is not very good. I’m not sure whether it is due to the problem of my ip.</li></ul></li><li>If want to have more ability of scraping, more IPs are needed in my program.(To be continued)</li></ul><h2 id="Connection-reset-by-peer"><a href="#Connection-reset-by-peer" class="headerlink" title="Connection reset by peer"></a>Connection reset by peer</h2><p>I’m not sure what’s the reason of this problem.</p><p>I add the proxies to avoid this problem. </p><ol><li><p>how to <a href="https://github.com/ShuoGH/web-crawler/blob/master/get_proxies.py" target="_blank" rel="noopener">get the proxies</a>.</p></li><li><p>Use it in the <code>requests.get</code> function. </p></li></ol><p>Although the program become slow, it become much more robust during scraping.</p>]]></content>
      
      
      <categories>
          
          <category> Web Crawler </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Web Crawler </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Web Crawler Basic</title>
      <link href="/2019/06/13/Web-Crawler-Basic/"/>
      <url>/2019/06/13/Web-Crawler-Basic/</url>
      
        <content type="html"><![CDATA[<p>There are several modules for accessing html through Python. </p><p><code>Urllib</code> and <code>requests</code> are two different modules in Python which can be used for crawler.</p><p>For beginner, the <code>requests</code> is recommended. </p><h2 id="Headers"><a href="#Headers" class="headerlink" title="Headers"></a>Headers</h2><p>HTTP request header is the information, in the form of a text record, that a user’s browser sends to a Web serveer containing the details of what the browser wants and will accept back from server.</p><h3 id="User-Agent"><a href="#User-Agent" class="headerlink" title="User-Agent"></a>User-Agent</h3><p>The User-Agent appears in an HTTP Request Header, not an HTTP Response one. In general, the request is sent from browser to the web application. So the user-agent is filled by the browser. Different browsers will fill up this field with different values.</p><p>Blog:  <a href="/2019/06/13/Web-Crawler-Get-the-User-Agent/" title="Web Crawler Get the User-Agent">Web Crawler Get the User-Agent</a></p><h3 id="Referer"><a href="#Referer" class="headerlink" title="Referer"></a>Referer</h3><p>Optional HTTP header field that identifies the address of the webpage that linked to the resource being requested. By checking the referee, the new webpage can see where the request originated.</p><p>Some websites use this to ban the crawler, and you may need to update your referer. </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://www.jianshu.com/p/1efa672156d3" target="_blank" rel="noopener">Python 爬虫基础之urllib和requests</a> </li><li>my demo of <a href="https://github.com/ShuoGH/web-crawler" target="_blank" rel="noopener">web crawler</a></li><li>stackoverflow: <a href="https://stackoverflow.com/questions/15069533/http-request-header-useragent-variable" target="_blank" rel="noopener">HTTP request header</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Web Crawler </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Web Crawler </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Association Rules Mining</title>
      <link href="/2019/05/29/Association-Rules-Mining/"/>
      <url>/2019/05/29/Association-Rules-Mining/</url>
      
        <content type="html"><![CDATA[<p>It also kind of Market Basket Analysis.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>An <strong>Association Rule</strong> is where <script type="math/tex">X \Longrightarrow Y</script> ($X$ implies $Y$)</p><p>An <strong>item set</strong> is a set of items. If it has $k$ items, it is a $k$-itemset.</p><p><strong>Support</strong> $s$ of an itemset $X$ is the percentage of transactions in $D$ that contains $X$. </p><p><strong>Support</strong> of <strong>association rule</strong> $X \Longrightarrow Y$ is the support of the itemset $\{X,Y\}$. </p><p><strong>Confidence</strong> of the rule $X \Longrightarrow Y$ is the ratio between the transactions that contain both $X$ and $Y$ and the number of transactions that have $X$ in $D$. </p><hr><p>The problem is: Find association rules.</p><p>Given: </p><ul><li>A set $I$ of items </li><li>database $D$ of transactions</li><li>minimum support $s$</li><li>minimum confidence $c$</li></ul><p>Find: Association rules $X \Longrightarrow Y$ with a minimum support $s$ and minimum confidence $c$ </p><hr><h2 id="Apriori-Algorithm"><a href="#Apriori-Algorithm" class="headerlink" title="Apriori Algorithm"></a>Apriori Algorithm</h2><p>There are two pinciples of the apriori algorithm:</p><ul><li>Any subset of a frequent itemset is also frequent </li><li>Any superset of an infrequent itemset is also infrequent</li></ul><p>(example see the reference 2)</p><h2 id="Improvements"><a href="#Improvements" class="headerlink" title="Improvements"></a>Improvements</h2><p>The limitation of <strong>confidence</strong>:</p><script type="math/tex; mode=display">\operatorname{conf}(X \Longrightarrow Y)=\frac{\frac{n T \operatorname{rans}(X \cup Y)}{|D|}}{\frac{n \operatorname{Trans}(X)}{|D|}}=\frac{p(X \wedge Y)}{p(X)}=p(Y | X)</script><p>If $Y$ is independent of $X$: <script type="math/tex">\mathrm{p}(Y)=\mathrm{p}(Y-X)</script>.</p><p>This means if you have a high probability of $p(Y)$ we have a rule with high confidence that associate independent itemset.</p><p><strong><em>Lift</em></strong>: measure indicates departure from independence of $X$ and $Y$.</p><p>The <strong>lift</strong> of $X \Longrightarrow Y$ is:</p><script type="math/tex; mode=display">\operatorname{lift}(X \Longrightarrow Y)=\frac{\operatorname{conf}(X \Longrightarrow Y)}{p(Y)}=\frac{\frac{p(X \wedge Y)}{p(X)}}{p(Y)}=\frac{p(X \wedge Y)}{p(X) p(Y)}</script><p>But lift is symmetric, the same for $X \Longrightarrow Y$ as $Y \Longrightarrow X$</p><p><strong><em>Conviction</em></strong>: indicates that $X$ and $Y$ are not independent, and takes into account the direction of implication. </p><p>Since the $p \rightarrow q \equiv \neg p \vee q$, and we can rewrite it as the $\neg( p \wedge \neg q)$. Therefore, the <strong>Conviction</strong> is based on this</p><script type="math/tex; mode=display">\operatorname{conv}(X \Longrightarrow Y)=\frac{p(X) p(\neg Y)}{p X \wedge \neg Y )}</script><p>Conviction is a measure of the implication and has value 1 if items are unrelated. </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Jo slide Market Basket</li><li><a href="https://paginas.fe.up.pt/~ec/files_0506/slides/04_AssociationRules.pdf" target="_blank" rel="noopener">Mining Association Rules</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Data Mining </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decision Tree</title>
      <link href="/2019/05/29/Decision-Tree/"/>
      <url>/2019/05/29/Decision-Tree/</url>
      
        <content type="html"><![CDATA[<p>A decision tree is like a flow chart.</p><p>One merit of using decision tree is that they are interpretable. It is easy to see how they made a certain decision. Besides, the decision trees can be “hand crafted” by experts. They can also be  built up using machine learning techniques.</p><h2 id="Criteria-for-Choosing-the-Split"><a href="#Criteria-for-Choosing-the-Split" class="headerlink" title="Criteria for Choosing the Split"></a>Criteria for Choosing the Split</h2><p>For classification:</p><ul><li><p>ID3: maximise the information gain (based on the information entropy). The information gain is kind of <strong>mutual entropy</strong>. See the <a href="https://shuogh.github.io/2019/05/25/Data-Mining-Information-Theory/" target="_blank" rel="noopener">blog</a> for more details.</p></li><li><p>CART: maximise the impurity decrease (based on the Gini impurity) </p><script type="math/tex; mode=display">\text {Gini}(r o o t)-\left(\operatorname{Gini}(L e f t) \frac{n_{L}}{n}+\operatorname{Gini}(\operatorname{Right}) \frac{n_{R}}{n}\right)</script><p>root is the node to be split, and left and right is the impurity of the left and right branches.</p></li></ul><p>For regression:</p><ul><li>CART: use variance instead of gini or entropy. Choose the feature decrease the variance most. </li></ul><p><strong>Note</strong>:</p><p>When computing the information gain, impurity decrease or the variance gain, remember to <strong>multiply the weights</strong> for the left and right trees.</p><h2 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h2><p>CART, the classification and regression tree. In this part, regression using the variance is going to be introduced.</p><p>The <strong>procedure</strong> of building the CART:</p><ul><li>Find the best split for each feature - minimises the impurity measure </li><li>Find feature that minimises impurity the most</li><li>Use the best split on that feature to split the node</li><li>Do the same for each of the leaf nodes</li></ul><p>（先找每个feature最好的区分点，比较所有feature的最好效果找出决定feature，然后split。不断迭代）</p><h2 id="Pruning"><a href="#Pruning" class="headerlink" title="Pruning"></a>Pruning</h2><p>The decision tree is easy to be overfitting. One solution is pruning. This can be done in a variety of ways, including:</p><ul><li>Reduced Error Pruning</li><li>Entropy Based Merging</li></ul><h3 id="Reduced-Error-Pruning"><a href="#Reduced-Error-Pruning" class="headerlink" title="Reduced Error Pruning"></a>Reduced Error Pruning</h3><p>Procedure:</p><ul><li>Start at leaf nodes</li><li>Look up branches at last decision split</li><li>Replace with a leaf node predicting the majority class</li><li>If validation set classification accuracy is not affected, the keep the change</li></ul><p>This is a simple and fast algorithm that can simplify over complex decision trees.</p><h3 id="Entropy-Based-Pruning"><a href="#Entropy-Based-Pruning" class="headerlink" title="Entropy Based Pruning"></a>Entropy Based Pruning</h3><p>Procedure:</p><ul><li>Chose a pair of leaf nodes with the same parent</li><li>What is the entropy gain from merging them </li><li>If lower than a threshold, merge nodes.</li></ul><p>This doesn’t require additional data.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Jo Slide Decision Tree</li></ol>]]></content>
      
      
      <categories>
          
          <category> Data Mining </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EM Algorithm</title>
      <link href="/2019/05/28/EM-Algorithm/"/>
      <url>/2019/05/28/EM-Algorithm/</url>
      
        <content type="html"><![CDATA[<p>EM algorithm is used to solve the problems which have the latent variable. Like the example mentioned in the <a href="https://zhuanlan.zhihu.com/p/36331115" target="_blank" rel="noopener">reference 1</a> about the heights of boys and girls.</p><p>Before explaining the EM algorith, we review the generative models. See the blog of <a href="https://shuogh.github.io/2019/05/25/Discriminative-and-Generative-Models/" target="_blank" rel="noopener">Discriminative and Generative models</a> and <a href="https://shuogh.github.io/2019/05/26/Naive-Bayes/" target="_blank" rel="noopener">Naive Bayes</a>. </p><h2 id="Building-Probabilistic-Models"><a href="#Building-Probabilistic-Models" class="headerlink" title="Building Probabilistic Models"></a>Building Probabilistic Models</h2><p>To describe a system with uncertainty we use random variables, $X$, $Y$, $Z$, etc.  </p><p>The variables are described by probability mass function $\mathbb{P}(X, Y, Z)$ or if our variables are continuous, but probability densities $f_{X, Y, Z}(x, y, z)$. We build in dependencies in this joint distribution.</p><p>We often think of our observations as given and the predictions as random variables. The object is to find a probability $\mathbb{P}(C | \boldsymbol{x})$. Modeling this directly is what we know as <strong>discriminative model</strong>. </p><p>In <strong>generative models</strong>, we think of the problem as the joint process of generating the features and outputs together. This leads to a joint distribution $\mathbb{P}(\boldsymbol{X}, Y)$ where $X$ are your features and $Y$ is your output you are trying to predict. Write  the approach like </p><script type="math/tex; mode=display">\mathbb{P}(Y | \boldsymbol{X})=\frac{\mathbb{P}(\boldsymbol{X}, Y)}{\mathbb{P}(\boldsymbol{X})}=\frac{\mathbb{P}(\boldsymbol{X}, Y)}{\sum_{Y} \mathbb{P}(\boldsymbol{X}, Y)}</script><h2 id="Latent-Variable"><a href="#Latent-Variable" class="headerlink" title="Latent Variable"></a>Latent Variable</h2><p><strong>Latent variables</strong>: The random variables involed in our models, but we don’t observe and we don’t care about.</p><p>If we have a latent variable $Z$ and observed variable $\boldsymbol{X}$ and we are predicting a variable $Y$ then we would marginalise over the latent variable</p><script type="math/tex; mode=display">\mathbb{P}(\boldsymbol{X}, Y)=\sum_{Z} \mathbb{P}(\boldsymbol{X}, Y, Z)</script><p>Suppose we were observing the decays from two types of short-lived particle. $X$ is our observation. Assume $X$ is normally distributed with unknown means and variances: $\Theta=\left\{\mu_{1}, \sigma_{1}^{2}, \mu_{2}, \sigma_{2}^{2}\right\}$. Let $Z\in \{0,1\}$ be an indicator that it is particle 1. </p><p>The probability of $X$ is given by </p><script type="math/tex; mode=display">f(X | Z, \mathbf{\Theta})=Z \mathcal{N}\left(X | \mu_{1}, \sigma_{1}^{2}\right)+(1-Z) \mathcal{N}\left(X | \mu_{2}, \sigma_{2}^{2}\right)</script><ul><li><p><strong>Note</strong>:</p><script type="math/tex; mode=display">\begin{aligned} f(X | \Theta) &=\sum_{Z \in\{0,1\}} f(X, Z | \Theta)=\sum_{Z \in\{0,1\}} f(X | Z, \Theta) \mathbb{P}(Z) \\ &=\mathbb{E}_{Z}[f(X | Z, \Theta)]=p \mathcal{N}\left(X | \mu_{1}, \sigma_{1}^{2}\right)+(1-p) \mathcal{N}\left(X | \mu_{2}, \sigma_{2}^{2}\right) \end{aligned}</script></li></ul><p>Then the likelihood of our observed data</p><script type="math/tex; mode=display">f(\mathcal{D} | \Theta)=\prod_{X \in \mathcal{D}} f(X | \Theta)</script><p>The likelihood function is a non-linear function of the parameters so cannot be immediately maximised. We have a difficulty in that our latent variable $Z$ will depend on the parameter $\Theta$, and our likelihood will depend on the latent variable.</p><p><strong>Solution</strong>:</p><script type="math/tex; mode=display">\Theta^{(t+1)}=\underset{\Theta}{\operatorname{argmax}} \sum_{Z} \mathbb{P}\left(Z | \mathcal{D}, \Theta^{(t)}\right) \log (f(\mathcal{D} | Z, \Theta))</script><p>proceed iteratively by maximising the <strong>expected log-likelihood</strong> with respect to the current set of parameters.</p><h2 id="EM-for-Mixture-of-Gaussians"><a href="#EM-for-Mixture-of-Gaussians" class="headerlink" title="EM for Mixture of Gaussians"></a>EM for Mixture of Gaussians</h2><p>(EM算法是求基于隐变量上的似然函数的期望。就像这样的形式$p\cdot log(f(D|Z,\Theta))+(1-p)\cdot log(f(D|Z,\Theta))$)</p><p>(to be continued)</p><p>The algorithm (Bishop PRML): </p><ol><li><p>Initialize the $\boldsymbol{\mu}_{k}$, covariance $\boldsymbol{\Sigma}_{k}$ and mixing coefficients $\pi_{k}$, and evaluate the initial value of the log likelihood.</p></li><li><p><strong>E step</strong>. Evaluate the responsibilities using the current parameter values</p><script type="math/tex; mode=display">\gamma\left(z_{n k}\right)=\frac{\pi_{k} \mathcal{N}\left(\mathbf{x}_{n} | \boldsymbol{\mu}_{k}, \mathbf{\Sigma}_{k}\right)}{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(\mathbf{x}_{n} | \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\right)}</script><p>（计算每个点n对应到k隐变量的概率）</p></li><li><p><strong>M step</strong>. Re-estimate the parameters using the current responsibilities. </p><script type="math/tex; mode=display">\begin{aligned} \boldsymbol{\mu}_{k}^{\mathrm{new}} &=\frac{1}{N_{k}} \sum_{n=1}^{N} \gamma\left(z_{n k}\right) \mathbf{x}_{n} \\ \boldsymbol{\Sigma}_{k}^{\mathrm{new}} &=\frac{1}{N_{k}} \sum_{n=1}^{N} \gamma\left(z_{n k}\right)\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}^{\mathrm{new}}\right)\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}^{\mathrm{new}}\right)^{\mathrm{T}} \\ \pi_{k}^{\mathrm{new}} &=\frac{N_{k}}{N} \end{aligned}</script><p>where </p><script type="math/tex; mode=display">N_{k}=\sum_{n=1}^{N} \gamma\left(z_{n k}\right)</script><p>（重新计算每类隐变量k的均值$\mu$和发那个方差矩阵$\Sigma$，以及重新分配隐变量分布的概率）</p></li><li><p>Evaluate the log likelihood</p><script type="math/tex; mode=display">\ln p(\mathbf{X} | \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})=\sum_{n=1}^{N} \ln \left\{\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\mathbf{x}_{n} | \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right\}</script><p>and check for convergence of either the parameters or the log likelihood. If the convergence criterion is not satisfied return to step 2.</p><p>（计算log 似然函数，这个在最开始也计算了一次。如果没有达到要求，重新迭代再进行计算）</p></li></ol><p>(not fully understand…)</p><h3 id="Example-of-Coin-Tossing"><a href="#Example-of-Coin-Tossing" class="headerlink" title="Example of Coin Tossing"></a>Example of Coin Tossing</h3><p><em>MLE</em>: there is no latent variable. The event are all belong to one distribution.</p><p><em>EM</em>: There are latent variables. And we don’t know the $p(Z)$ where $Z$ is the latent variable. </p><p>(See the example mentioned in reference 1)</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>zhihu <a href="https://zhuanlan.zhihu.com/p/36331115" target="_blank" rel="noopener">人人都懂EM算法</a></li><li>adam slide <a href="https://secure.ecs.soton.ac.uk/notes/comp6208/lectures/generativeModels.pdf" target="_blank" rel="noopener">Generative Models</a></li><li>Bishop PRML Chapter 9.2</li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bayes Rule, Prior and Posterior</title>
      <link href="/2019/05/27/Bayes-Rule-Prior-and-Posterior/"/>
      <url>/2019/05/27/Bayes-Rule-Prior-and-Posterior/</url>
      
        <content type="html"><![CDATA[<h2 id="Bayes’-Rule"><a href="#Bayes’-Rule" class="headerlink" title="Bayes’ Rule"></a>Bayes’ Rule</h2><p>The form of bayes’ rule is </p><script type="math/tex; mode=display">\mathbb{P}\left(\mathcal{H}_{i} | \mathcal{D}\right)=\frac{\mathbb{P}\left(\mathcal{D} | \mathcal{H}_{i}\right) \mathbb{P}\left(\mathcal{H}_{i}\right)}{\mathbb{P}(\mathcal{D})}</script><ul><li><p>$\mathbb{P}\left(\mathcal{H}_{i} | \mathcal{D}\right)$ is the <strong>posterior</strong> probability of a hypothesis $\mathcal{H}_{i}$ (i.e. the probability of $\mathcal{H}_{i}$ after we know the data)</p></li><li><p>$\mathbb{P}\left(\mathcal{D} | \mathcal{H}_{i}\right)$ is the <strong>likelihood</strong> of the data given the hypothesis. Note, that we calculated this from the forward problem</p></li><li><p>$\mathbb{P}\left(\mathcal{H}_{i}\right)$ is the <strong>prior</strong> probability (i.e. the probability of $\mathcal{H}_{i}$ before we know the data)</p></li><li><p>$\mathbb{P}(\mathcal{D})$ is the <strong>evidence</strong>. It is the normalising constant given by </p><script type="math/tex; mode=display">\mathbb{P}(\mathcal{D})=\sum_{i=1}^{n} \mathbb{P}\left(\mathcal{H}_{i}, \mathcal{D}\right)=\sum_{i=1}^{n} \mathbb{P}\left(\mathcal{D} | \mathcal{H}_{i}\right) \mathbb{P}\left(\mathcal{H}_{i}\right)</script></li></ul><p>In most of our task, what we want is the posterior probability.</p><p>The bayes’ rule converts this into the forward problem.</p><h1 id="Prior-and-Posterior"><a href="#Prior-and-Posterior" class="headerlink" title="Prior and Posterior"></a>Prior and Posterior</h1><p>(to be continued)</p><p>When the posterior is the same as the prior then the likelihood and prior distributions are said to be <strong>conjugate</strong>. The prior then is the conjugate prior. </p><p>In the xxxxxx, we want to maximize the posterior probabilities(MAP), which is different with the fully bayesian inference.(2017-2018 exam paper AML)</p><h2 id="MAP-and-MLE"><a href="#MAP-and-MLE" class="headerlink" title="MAP and MLE"></a>MAP and MLE</h2><p>MAP: maximize the posterior distribution </p><p>MLE: maximum likelihood estimation </p><p>In MAP, we should add a prior distribution and by adding observations and then to maximize the posterior distribution $p(w|X)$ to get the parameter. </p><p>最大似然估计是求参数$\theta$, 使似然函数$P(x_0|\theta)$最大。最大后验概率估计则是想求$\theta$使$P(x_0|\theta)P(\theta)$最大,求得的$ \theta $不单单让似然函数大，$ \theta $自己出现的先验概率(也是得到的后验概率）也得大.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Bishop PRML</li><li><a href="ttps://blog.csdn.net/u011508640/article/details/72815981" target="_blank" rel="noopener">详解最大似然估计（MLE）、最大后验概率估计（MAP)</a></li><li>Adam slide bayes_prn comp6208</li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LSTM</title>
      <link href="/2019/05/27/LSTM/"/>
      <url>/2019/05/27/LSTM/</url>
      
        <content type="html"><![CDATA[<p><img src="https://lh3.googleusercontent.com/1a3JemZmq0O2oLdO-PWxDmmCKtqiIpYQv5mZHqjn7qyLUyd3uKrEMG95JIguwhtjGubeF2aSeEhuIKeD9YpZUabM4w8tjLQbQuieqyjwAetTlAYPdoED31q2UIMf6OrrL7V22qj3HMVkd_uTrYvUDMWYSl-ii3N95xHYoAVs8yzOrXGM7DVj_6VxGINnM-pGU1QseXuvyF0he3C4YQd-ksUnnlTfDAAQjxBcubfCpIMbi5StM-F1zq8GJuGqJq4AzbNv5FP8SZX17xbV-RpGZdMhMCIVLHrIIR2QdRrYLivdKz-9v9IULAxt8vv96rBaT8yn3KRDr_Ir5odQjnW6gVISynBsKJ4xMFMqMwISq-PGn75nzkXKZqr3aPO9nqm6uMe13lFQ1iW4-aXiHQcNIjLhUiufOIbmBuLoNxoiEfZXF-nFkKp7QNWoCcAfkzuyJDHD81B6HIE2e1eQM-bLdomLShk8f7nqJo4x7RoNk3Sw_pbXTkO4rYS7prEabrZsjqRjF15EQYU74jyRuNDU3AD8shfn1S0NPEhF3hsQILBvm67RJyQePWhhcgsAxPnh4ggVDpmaoxWxRzNTttsVd89j0SZ5w5OdKKs95ZDMOZob2MAp3cvRbyYZqrupydx2piO8D8hp0TI2BMOOELPZcvRFVfDepA=w1476-h1078-no" alt="avatar"></p><p>The Structure of the LSTM, which is shown in the image above.</p><h2 id="Items-in-the-Structure"><a href="#Items-in-the-Structure" class="headerlink" title="Items in the Structure"></a>Items in the Structure</h2><ul><li><p>Inputs $\boldsymbol{z}(t)=(\boldsymbol{x}(t), \boldsymbol{y}(t-1))$</p><p>The input is combined the input $x(t)$ and the output of last time $y(t-1)$</p></li><li><p>Network Updates ($W_*$ are the free parameters)</p><script type="math/tex; mode=display">\begin{array}{ll}{\boldsymbol{f}(t)=\boldsymbol{\sigma}\left(\mathbf{W}_{f} \boldsymbol{z}(t)\right)} & {\boldsymbol{g}(t)=\boldsymbol{\sigma}\left(\boldsymbol{W}_{g} \boldsymbol{z}(t)\right)} \\ {\boldsymbol{h}(t)=\tanh \left(\boldsymbol{W}_{h} \boldsymbol{z}(t)\right)} & {\boldsymbol{o}(t)=\boldsymbol{\sigma}\left(\boldsymbol{W}_{o} \boldsymbol{z}(t)\right)}\end{array}</script><p>$\boldsymbol f(t)$ is the forget gate, $\boldsymbol g(t)$ decide the whether and what to remember from the input of this time, the $\boldsymbol h(t)$ is the input block, the $\boldsymbol o(t)$ is the output gate.</p></li><li><p>Long-term memory update </p><script type="math/tex; mode=display">\boldsymbol{c}(t)=\boldsymbol{f}(t) \otimes \boldsymbol{c}(t-1)+\boldsymbol{g}(t) \otimes \boldsymbol{h}(t)</script><p>It can be seen as the influence of the forget gate on the hidden state plus the memory update.<br>(对隐含变量是否进行遗忘，加上这次的选择记忆阶段得到下一次的隐含状态的输入$c(t)$)</p></li><li><p>Output $\boldsymbol{y}(t)=\boldsymbol{o}(t) \otimes \tanh (\boldsymbol{c}(t))$ </p><p>The product of the output gate and the $tanh(\boldsymbol c(t))$.</p></li></ul><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>We can train an LSTM by unwrapping it in time.</p><p>It involves four dense layers with sigmoidal(or tanh) output: those gates </p><p>The LSTM is typucally very slow to train.</p><p>There are a few variants of LSTMs, but all are very similar. The most popular is probably <em>Gated Recurrent Unit</em> (GRU).</p><p>(to be continued)</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Adam lstm slide COMP6208</li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kernel Tricks</title>
      <link href="/2019/05/27/Kernel-Tricks/"/>
      <url>/2019/05/27/Kernel-Tricks/</url>
      
        <content type="html"><![CDATA[<h2 id="Kernels"><a href="#Kernels" class="headerlink" title="Kernels"></a>Kernels</h2><p>Many linear parametric models can be re-written into an equivalent <em>dual representation</em> in which the predictions are also based on linear combinations of a kernel function evaluated at the training data points. As we shall see, for models which are based on a fixed nonlinear feature space mapping $\phi(x)$, the <strong>kernel</strong> function is given by the relation</p><script type="math/tex; mode=display">k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}^{\prime}\right)</script><p><strong>Dual Representation and Kernel</strong>: See an classical example of SVM from this <a href="https://shuogh.github.io/2019/05/26/SVM-Slack-Variable-and-Kernel/" target="_blank" rel="noopener">blog</a>.</p><p>Some common forms of kernels:</p><ul><li><p>Linear Kernel 线性核 $\kappa \left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) =\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}$</p></li><li><p>Polynomial Kernel 多项式核 $\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) =\left(\boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}\right)^{d}$</p></li><li><p>Gaussian Kernel 高斯核 $\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) =\exp \left(-\frac{\left|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right|^{2}}{2 \sigma^{2}}\right)$</p></li><li><p>Laplace Kernel 拉普拉斯核 $\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) =\exp \left(-\frac{\left|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right|}{\sigma}\right)$</p></li><li><p>Sigmoid Kernel $\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) =\tanh \left(\beta \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j}+\theta\right)$</p></li></ul><p>By replacing the $-\frac{1}{2\sigma^2}$ with $\lambda$ in the Gaussian Kernel, we can get the RBF kernel, which can be express as $\exp \left(-\gamma|\boldsymbol{x}-\boldsymbol{y}|^{2}\right)$.</p><p><strong>Note</strong> that the feature vector that corresponds to the Gaussian kernel has infinite dimensionality.</p><h2 id="Kernel-Tricks"><a href="#Kernel-Tricks" class="headerlink" title="Kernel Tricks"></a>Kernel Tricks</h2><p>So, what is kernel?</p><p>In machine learning, a <strong>kernel</strong> is usually used to refer to the kernel trick, a method of using a linear classifier to solve a non-linear problem.</p><p>The kernel trick allows us to map data into high-dimensional feature space $\boldsymbol{x} \rightarrow \phi(\boldsymbol{x})$. This can be carried out for sufficiently simple machines where parameter optimisation involve the dot product $\phi^{\mathbf{T}}(\boldsymbol{x}) \phi(\boldsymbol{y})$. The kernel function is positive semi-definite and the decomposition is always possible (see the properties in the end of this blog). In fact, we never need to explicitly calculate the extended features $\phi(\boldsymbol{x})$. This often makes working in the extended feature space very efficient as $K(\boldsymbol{x},\boldsymbol{y})$ may be quick to calculate.</p><h2 id="Constructing-Kernels"><a href="#Constructing-Kernels" class="headerlink" title="Constructing Kernels"></a>Constructing Kernels</h2><p>One approach to build valid kernel is to choose a feature space mapping $\phi(x)$ and use it to find the corresponding kernel. Here the kernel function is defined for one-dimensional input space by </p><script type="math/tex; mode=display">k\left(x, x^{\prime}\right)=\phi(x)^{\mathrm{T}} \boldsymbol{\phi}\left(x^{\prime}\right)=\sum_{i=1}^{M} \phi_{i}(x) \phi_{i}\left(x^{\prime}\right)</script><p>where $\phi(x)$ is the basis function. </p><p>Another approach is to construct the kernel directly. In this way, we should have a method to test whether the kernel we build is valid. A <strong>necessary and sufficient condition</strong> for a function $k(x,x)$ to be a valid kernel is that the <strong>Gram</strong> matrix $K$, whose elements are given by $k(x,x)$, should be positive semi-definite for all possible choices of the set $x_n$. Note that a positive semi-definite matrix is not the same thing as a matrix whose elements are nonnegative.</p><p>One powerful technique for constructing new kernels is to build out of simple kernels as building blocks. See the properties from the PRML book in $P_{296}$.</p><hr><h2 id="Some-questions-about-the-Kernel"><a href="#Some-questions-about-the-Kernel" class="headerlink" title="Some questions about the Kernel"></a>Some questions about the Kernel</h2><ol><li>Why it is important that the kernel function is positive semi-definite?<br> Kernel functions need to be positive semi-definite so that they have sensible (nonnegative) distances. That is the margins are positive.</li><li>Three properties that a positive semi-definite kernel should have:<ul><li>The eigenvalues of a positive semi-definite kernel function are nonnegative.</li><li>A positive semi-definite kernel function can always be written as <script type="math/tex; mode=display">K(x,y)=\sum_i \phi_i(x)\phi_i(y)</script>for some sets of real functions $\phi_i(x)$</li><li>The quadratic form satisfies <script type="math/tex; mode=display">\int f(\boldsymbol{x}) K(\boldsymbol{x}, \boldsymbol{y}) f(\boldsymbol{y}) \mathrm{d} \boldsymbol{x} \mathrm{d} \boldsymbol{y} \geq 0</script>for any real function $f(x)$.</li></ul></li><li>Why kernel trick allows an SVM to seperate data points that are not linearly separable?<br> The kernel trick projects data into the extended feature space (the space of engenfunctions of the kernel). Although an SVM finds a linear separating plane in this extended space, as the extended features are typically a non-linear function of the original features. This corresponds to finding a non-linear separating surface in the original space.</li></ol><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>周志华 《机器学习》</li><li>Bishop PRML Chapter 6</li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo Local Search not Respond</title>
      <link href="/2019/05/27/Hexo-Local-Search-not-Respond/"/>
      <url>/2019/05/27/Hexo-Local-Search-not-Respond/</url>
      
        <content type="html"><![CDATA[<p>When your article have some unlegal characters, the search function will not work.<br>See the <a href="https://segmentfault.com/q/1010000013084615" target="_blank" rel="noopener">https://segmentfault.com/q/1010000013084615</a> discussion for more details.</p><p>To debug, go to the url to see which blog rise this issue. <a href="http://localhost:4000/search.xml" target="_blank" rel="noopener">http://localhost:4000/search.xml</a></p><p>(通过注释并且在本地调试，发现是kernels trick这个文件有问题。)</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>应该是有非法字符，local search无法加载出来非utf-8编码的字符。通过注释排除法，找到问题根源。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://segmentfault.com/q/1010000013084615" target="_blank" rel="noopener">hexo的local search不能使用</a></li><li><a href="https://sherlockgy.github.io/2018/06/09/Hexo%E6%9C%AC%E5%9C%B0%E6%90%9C%E7%B4%A2%E5%A4%B1%E6%95%88%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/" target="_blank" rel="noopener">Hexo本地搜索失效解决办法</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bias and Variance</title>
      <link href="/2019/05/27/Bias-and-Variance/"/>
      <url>/2019/05/27/Bias-and-Variance/</url>
      
        <content type="html"><![CDATA[<h2 id="Bias-and-Variance"><a href="#Bias-and-Variance" class="headerlink" title="Bias and Variance"></a>Bias and Variance</h2><p>A good learner classifier should have a good <strong>generalisation error</strong>.     </p><ul><li>Generalisation: how well do we do on unseen data as opposed to the training data</li></ul><p>The problems in the Machine Learning can be over-constrained and under-constrained.</p><ul><li>Over-constrained: We have conflicting data to deal with. There are more equations than variables. In this case, the learner has insufficient flexibility to correctly predict all the training data. To solve this problem, we can minimise an error function, which means that we find a machine that explained the training data as best it can.</li><li>Under-constrained: There are many possible solutions that are consistent with the data . Need to choose a plausible solution.</li></ul><p><strong>Bias</strong>: the generalisation performance of the mean machine. </p><script type="math/tex; mode=display">\hat{f}_{m}(\boldsymbol{x})=\mathbb{E}_{\mathcal{D}}\left[\hat{f}\left(\boldsymbol{x} | \boldsymbol{w}_{\mathcal{D}}\right)\right]</script><p>which $\hat{f}_{m}(\boldsymbol{x})$ is the mean predictor(machine) value. And the bias is defined as</p><script type="math/tex; mode=display">B=\sum_{x \in \mathcal{X}} p(\boldsymbol{x})\left(\hat{f}_{m}(\boldsymbol{x})-f(\boldsymbol{x})\right)^{2}</script><p>（可以看成最终训练出的分类器在训练集上进行预测，得到的所有值的平均值与每一个原target计算error）</p><p><strong>Variance</strong>: measures the expected variation from the average machine due to the fluctuations caused by the using a finite training set.</p><script type="math/tex; mode=display">V=\mathbb{E}_{\mathcal{D}}\left[\sum_{x \in \mathcal{X}} p(\boldsymbol{x})\left(\hat{f}\left(\boldsymbol{x} | \boldsymbol{w}_{\mathcal{D}}\right)-\hat{f}_{m}(\boldsymbol{x})\right)^{2}\right]</script><p>（训练出来的分类器，对每个训练集的输入进行预测，计算这些值的分布的方差）</p><h2 id="Decomposition"><a href="#Decomposition" class="headerlink" title="Decomposition"></a>Decomposition</h2><p>The formulas of bias and variance are already defined above. Here we are going to show the decomposition.</p><p>The expected generalisation（平均泛化误差） is written as</p><script type="math/tex; mode=display">\overline{E}_{G}=\mathbb{E}_{\mathcal{D}}\left[E_{G}\left(\boldsymbol{w}_{\mathcal{D}}\right)\right]=\mathbb{E}_{\mathcal{D}}\left[\sum_{\boldsymbol{x} \in \mathcal{X}} p(\boldsymbol{x})\left(\hat{f}\left(\boldsymbol{x} | \boldsymbol{w}_{\mathcal{D}}\right)-f(\boldsymbol{x})\right)^{2}\right]</script><script type="math/tex; mode=display">=\sum_{\boldsymbol{x} \in \mathcal{X}} p(\boldsymbol{x}) \mathbb{E}_{\mathcal{D}}\left[\left(\hat{f}\left(\boldsymbol{x} | \boldsymbol{w}_{\mathcal{D}}\right)-f(\boldsymbol{x})\right)^{2}\right]</script><script type="math/tex; mode=display">=\sum_{\boldsymbol{x} \in \mathcal{X}} p(\boldsymbol{x}) \mathbb{E}_{\mathcal{D}}\left[\left(\left(\hat{f}\left(\boldsymbol{x} | \boldsymbol{w}_{\mathcal{D}}\right)-\hat{f}_{m}(\boldsymbol{x})\right)+\left(\hat{f}_{m}(\boldsymbol{x})-f(\boldsymbol{x})\right)\right)^{2}\right]</script><script type="math/tex; mode=display">\begin{aligned}=\sum_{\boldsymbol{x} \in \mathcal{X}} p(\boldsymbol{x})( & \mathbb{E}_{\mathcal{D}}\left[\left(\hat{f}\left(\boldsymbol{x} | \boldsymbol{w}_{\mathcal{D}}\right)-\hat{f}_{m}(\boldsymbol{x})\right)^{2}+\left(\hat{f}_{m}(\boldsymbol{x})-f(\boldsymbol{x})\right)^{2}\right] \\ &+\mathbb{E}_{\mathcal{D}}\left[2\left(\hat{f}\left(\boldsymbol{x} | \boldsymbol{w}_{\mathcal{D}}\right)-\hat{f}_{m}(\boldsymbol{x})\right)\left(\hat{f}_{m}(\boldsymbol{x})-f(\boldsymbol{x})\right)\right] ) \end{aligned}</script><p>The second term will vanish sicne the $\hat{f}_{m}(\boldsymbol{x})=\mathbb{E}_{\mathcal{D}}\left[\hat{f}\left(\boldsymbol{x} | \boldsymbol{w}_{\mathcal{D}}\right)\right]$. Finally we can rewrite the generalisation formula as </p><script type="math/tex; mode=display">\begin{aligned} \mathbb{E}_{\mathcal{D}}\left[E_{G}\left(\boldsymbol{w}_{\mathcal{D}}\right)\right]=\mathbb{E}_{\mathcal{D}} &\left[\sum_{\boldsymbol{x} \in \mathcal{X}} p(\boldsymbol{x})\left(\hat{f}\left(\boldsymbol{x} | \boldsymbol{w}_{\mathcal{D}}\right)-\hat{f}_{m}(\boldsymbol{x})\right)^{2}\right] \\+\sum & \sum_{\boldsymbol{x} \in \mathcal{X}} p(\boldsymbol{x})\left(\hat{f}_{m}(\boldsymbol{x})-f(\boldsymbol{x})\right)^{2}=V+B \end{aligned}</script><h2 id="Bias-Variance-Dilemma"><a href="#Bias-Variance-Dilemma" class="headerlink" title="Bias-Variance Dilemma"></a>Bias-Variance Dilemma</h2><p>The composition mentioned above encodes how sensitive the machine is to the data. </p><p>The dilemma arises because a simple machine will typically have a large bias, but small variance, while a complicated machine will have a small bias but large variance.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Adam slide01 COMP6208</li><li>Additional reading Bishop PRML Chapter 3.2</li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Radial Basis Function, RBF network</title>
      <link href="/2019/05/27/Radial-Basis-Function-RBF-network/"/>
      <url>/2019/05/27/Radial-Basis-Function-RBF-network/</url>
      
        <content type="html"><![CDATA[<h2 id="Radial-basis-function"><a href="#Radial-basis-function" class="headerlink" title="Radial basis function"></a>Radial basis function</h2><p><em>radial basis function</em> is that each basis function depends only on the radial distance (typically Euclidean) from a centre $\mu_j$, so that $\phi_j(x)=h(x-\mu_j)$.</p><p>$f(x)$ is expressed as a linear combination of radial basis functions, one<br>centred on every data point</p><script type="math/tex; mode=display">f(\mathbf{x})=\sum_{n=1}^{N} w_{n} h\left(\left\|\mathbf{x}-\mathbf{x}_{n}\right\|\right)</script><p>The values of the coefficients ${w_n}$ are found by least squares.</p><h2 id="RBF-Network"><a href="#RBF-Network" class="headerlink" title="RBF Network"></a>RBF Network</h2><p>(to be continued)</p><p><strong>Question: What are the similarities and differences between MLP, RBF networks and SVMs? </strong></p><ul><li>All three techniques are based on the perceptron. In MLPs, the earlier layers are perceptrons, in RBFs they are radial basis functions and SVMs thet are the features corresponding to the eigenvalues of a kernel.</li><li>All three can be used for regression and classification.</li><li>MLPs are trained using back-propagation of errors. They have non-unique solution. Complexity depends on nunber of hidden nodes. Liable to over-fit the training data. Often use ad hoc methods such as early stopping to stop over-fitting. Can have many output nodes.</li><li>RBFs typically use unsupervised learning to choose the centres for the input layer. The labelled data used to train the final layer(a perceptron). Training is fast. Can have many output nodes. Often use regulariser on the output layer. The solution found is unique. </li><li>SVMs use a kernel function to perform a mapping into a very high dimensional feature space. An optimally stable perceptron is used in the feature space. This controls the capacity of the learning machine reducing the problem of over-fitting. The learning algorithm uses quadratic optimisation. The computation complexity grows as the number training patterns cubed. For very large datasets SVMs can become impractical. The solution found is unque.</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>ML学习笔记之——<a href="http://ghx0x0.github.io/2015/06/11/ML-RBFnet/" target="_blank" rel="noopener">径向基网络</a></li><li>Bishop PRML chapter 6.3</li><li>question from (COMP3008 2009-2010 Q4)</li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVM, Slack Variable and Kernel</title>
      <link href="/2019/05/26/SVM-Slack-Variable-and-Kernel/"/>
      <url>/2019/05/26/SVM-Slack-Variable-and-Kernel/</url>
      
        <content type="html"><![CDATA[<h2 id="Maximize-margin"><a href="#Maximize-margin" class="headerlink" title="Maximize margin"></a>Maximize margin</h2><p>The basic idea of SVM is to maximize the margin between the support vectors and the hyperplane. </p><p>The margin can be expressed as $r=\frac{\left|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right|}{|\boldsymbol{w}|}$</p><p>For the binary classification task $y\in(-1,+1)$, the equation satisfies: </p><script type="math/tex; mode=display">y_i(w^Tx_i+b)\geqslant r\|w\|</script><p>Since the $w$ and $b$ can be scaled, we can get the condition </p><script type="math/tex; mode=display">y_i(w'^Tx_i+b')\geqslant 1</script><p>where $w’=\frac{w}{r}, b’=\frac{b}{r}$. </p><p>In the maximizing margin process, we only consider the support vectors which locate on the margin of hyperplane. These vectors satisfy the equation, therefore, the margin can be written as $\gamma=\frac{2}{|\boldsymbol{w}|}$. </p><p>Maximizing the margin is equivalent to </p><script type="math/tex; mode=display">\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}</script><script type="math/tex; mode=display">\text { s.t. } \quad y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1, \quad i=1,2, \ldots, m</script><h2 id="Lagrangian"><a href="#Lagrangian" class="headerlink" title="Lagrangian"></a>Lagrangian</h2><p>To solve the optimization problem, apply the Lagrangian multiplier, and we get the form of the question:</p><script type="math/tex; mode=display">L(w,b,\alpha)=\frac{1}{2}\|w\|^2-\sum_i^p \alpha_i(y_i(w^Tx_i+b-1))</script><script type="math/tex; mode=display">\begin{array}{l}{\text { s.t. } \sum_{i=1}^{m} \alpha_{i} y_{i}=0} \\ {\alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m}\end{array}</script><h2 id="Dual-Form"><a href="#Dual-Form" class="headerlink" title="Dual Form"></a>Dual Form</h2><p>Before applying the kernel functions, the dual representations are introduced.</p><p>Using the lagrandian multiplier, we convert our problem from $\min _{\boldsymbol{w}, b} \frac{1}{2}|\boldsymbol{w}|^{2}$ to $\min _{\boldsymbol{w}, b}\max_{\boldsymbol{\alpha}} L(w,b,\alpha)$. And it is equivalent to  </p><script type="math/tex; mode=display">\max_{\boldsymbol{\alpha}}\{ \min _{\boldsymbol{w}, b} L(w,b,\alpha)\}</script><p>To minimize the inner loss function, we calculate the partial derivative of $w$ and $b$, and make them equal to $0$, an then we get the the </p><script type="math/tex; mode=display">\begin{aligned} \boldsymbol{w}=& \sum_{i=1}^{m} \alpha_{i} y_{i} \boldsymbol{x}_{i} \\ 0 &=\sum_{i=1}^{m} \alpha_{i} y_{i} \end{aligned}</script><p>Substitute the results back to the origin $L(w,b,\alpha)$ loss function, now the dual representation is </p><script type="math/tex; mode=display">\max _{\boldsymbol{\alpha}} \left ( \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \boldsymbol{x}_{i}^{\mathrm{T}} \boldsymbol{x}_{j} \right)</script><script type="math/tex; mode=display">\begin{array}{l}{\text { s.t. } \sum_{i=1}^{m} \alpha_{i} y_{i}=0} \\ {\alpha_{i} \geqslant 0, \quad i=1,2, \ldots, m}\end{array}</script><p>Then we can apply the kernel.</p><p>Assum the kernel function which has the form like this </p><script type="math/tex; mode=display">\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left\langle\phi\left(\boldsymbol{x}_{i}\right), \phi\left(\boldsymbol{x}_{j}\right)\right\rangle=\phi\left(\boldsymbol{x}_{i}\right)^{\mathrm{T}} \phi\left(\boldsymbol{x}_{j}\right)</script><p>Write the matrix form $K=\boldsymbol\phi(\boldsymbol x)\boldsymbol\phi(\boldsymbol y)$, and rewrite the dual representation as the matrix form </p><script type="math/tex; mode=display">\boldsymbol c^T \boldsymbol\alpha-\frac{1}{2}\boldsymbol\alpha^T K \boldsymbol\alpha</script><p>with the constraints.</p><hr><p>The advantage of using the kernel trick: blog <a href="https://shuogh.github.io/2019/05/27/Kernel-Tricks/" target="_blank" rel="noopener">Kernel Function</a></p><p>Some common form of kernels: blog <a href="https://shuogh.github.io/2019/05/27/Kernel-Tricks/" target="_blank" rel="noopener">Kernel Function</a></p><hr><h2 id="Slack-Variable"><a href="#Slack-Variable" class="headerlink" title="Slack Variable"></a>Slack Variable</h2><p>对于不能完全分隔开的情况，引入松弛变量，使硬间隔(hard margin)转换成弱间隔(soft margin)。落在软间隔中的点才是我们要关注的东西，所以之后的slack variable只有在间隔内的才是不等于0的。</p><p>The form of classification constraints are replaced with</p><script type="math/tex; mode=display">\begin{array}{c}{\text { s.t. } \quad y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right) \geqslant 1-\xi_{i}} \\ {\xi_{i} \geqslant 0, i=1,2, \ldots, m}\end{array}</script><p>Each slack variable is for one training data point. </p><p>Data points with $ \xi_n=0$ are correctly classified and are either on the margin or on the correct side of the margin. Points for which $0&lt;\xi_{n} \leqslant 1$ lie inside the margin, but on the correct boundary, and those data points for which $\xi_n&gt;1$ lie on the wrong side of the decision boundary and are misclassified. </p><p>Slack variable allow for overlapping class distributions, however this framework is still sensitive to outliers because the penalty for misclassification increase linearly with $\xi$. (错误分类的离群点，会有很大的 $\xi$ 绝对值)</p><p>Therefore, the function we are going to <strong>minimize</strong> is </p><script type="math/tex; mode=display">C \sum_{n=1}^{N} \xi_{n}+\frac{1}{2}\|\mathbf{w}\|^{2}</script><p>where the parameter $C&gt;0$ controls the trade-off between the salck variable penality and the margin. Because any point that is misclassified has $\xi_n&gt;1$, it follows that the $\sum_{n} \xi_{n}$ is an upper bound on the number of misclassified points. Therefore, the parameter $C$ is analogous to a regularization coefficient because it controls the trade-off between minimizing training errors and controlling the model complexity. In the limit $C \rightarrow \infty$, we will recover the earlier SVM with the hard margin. </p><p>The corresponding Lagrangian is given by</p><script type="math/tex; mode=display">L(\mathbf{w}, b, \mathbf{a})=\frac{1}{2}\|\mathbf{w}\|^{2}+C \sum_{n=1}^{N} \xi_{n}-\sum_{n=1}^{N} a_{n}\left\{t_{n} y\left(\mathbf{x}_{n}\right)-1+\xi_{n}\right\}-\sum_{n=1}^{N} \mu_{n} \xi_{n}</script><p>and there are the constraints(see the book)</p><p>We can also convert it to the dual form and during the calculating the derivative, we get the another constraint $0 \leqslant a_{n} \leqslant C$. </p><p><code>(since there are a lot of content which can be written down. I will omit these information. If you want to get to know more, see the Chapter 7.1.1 of PRML of Bishop)</code></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>周志华 《机器学习》</li><li>Bishop PRML Chapter 7.1.1</li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Matrix Derivatives</title>
      <link href="/2019/05/26/Matrix-Derivatives/"/>
      <url>/2019/05/26/Matrix-Derivatives/</url>
      
        <content type="html"><![CDATA[<h2 id="Common-Used-in-Machine-Learning"><a href="#Common-Used-in-Machine-Learning" class="headerlink" title="Common Used in Machine Learning"></a>Common Used in Machine Learning</h2><p>There are two types of layout in the expression of matrix derivatives: denominator and numberator layout.</p><p>Here we always use the denominator layout.</p><ol><li>vector to vector</li></ol><script type="math/tex; mode=display">\frac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}}=\mathbf{A}^{\top}</script><script type="math/tex; mode=display">\frac{\partial \mathbf{x}^{\top} \mathbf{A}}{\partial \mathbf{x}}= \mathbf{A}</script><ol><li>scaler to vector</li></ol><script type="math/tex; mode=display">\frac{\partial \mathbf{x}^{\top} \mathbf{A} \mathbf{x}}{\partial \mathbf{x}}=\left(\mathbf{A}+\mathbf{A}^{\top}\right) \mathbf{x}</script><script type="math/tex; mode=display">\frac{\partial \mathbf{x}^{\mathbf{T}} \mathbf{x}}{\partial \mathbf{x}}=2 \mathbf{x}</script><p>These two conclusions are very popular in the derivative calculation of Machine Learning.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>矩阵求导、几种重要的矩阵及常用的<a href="https://blog.csdn.net/daaikuaichuan/article/details/80620518" target="_blank" rel="noopener">矩阵求导公式</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Linear Algebra </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linear Algebra </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Naive Bayes</title>
      <link href="/2019/05/26/Naive-Bayes/"/>
      <url>/2019/05/26/Naive-Bayes/</url>
      
        <content type="html"><![CDATA[<h2 id="What-is-Generative-Models"><a href="#What-is-Generative-Models" class="headerlink" title="What is Generative Models"></a>What is Generative Models</h2><p>See the last article: <code>discriminative models and generative models</code>.</p><p>The Naive Bayes belongs to the generative models, which model the distribution of the posterior and the process of generating the inputs.</p><h2 id="Assumption-of-Naive-Bayes"><a href="#Assumption-of-Naive-Bayes" class="headerlink" title="Assumption of Naive Bayes"></a>Assumption of Naive Bayes</h2><p>The naive bayes assumption is that all the data is conditionally independent, so if $D=(d_i|i=1,…,n)$ then </p><script type="math/tex; mode=display">p(\mathcal{D} | \boldsymbol{\theta})=\prod_{i=1}^{n} p\left(d_{i} | \boldsymbol{\theta}\right)</script><p>(which also shown in the PRML P46)</p><h2 id="Example-of-implementing-spam-filter"><a href="#Example-of-implementing-spam-filter" class="headerlink" title="Example of implementing spam filter"></a>Example of implementing spam filter</h2><p>To implement a spam filter we can treat all the words in the email as independent of each other. Given an email $\left\langle w_{1}, w_{2}, \dots, w_{n}\right\rangle$ we can compute the probability of it being spam as </p><script type="math/tex; mode=display">p(\operatorname{spam} | \mathcal{D})=\frac{\prod_{i=1}^{n} p\left(w_{i} | \operatorname{spam}\right) p(\operatorname{spam})}{p(\mathcal{D})}</script><p>where the $p(spam)$ is the empirically measured frequency of spam emails. To compute the likelihood we use a database of spam and non spam emails</p><script type="math/tex; mode=display">p\left(w_{i} | s p a m\right)=\frac{\# \text { of occurances of } w_{i} \text { in spam database }}{\# \text { of words in spam database }}</script><p>Here I use the assumption we mentioned above, the likelihood $p(D|spam)$ is defined by the multiplication of each $p(w_i|spam)$. (We might include pseudo counts to make this more robust). The probability of the data $D$ is</p><script type="math/tex; mode=display">p(\mathcal{D})=p(\mathcal{D} | \operatorname{spam}) p(\operatorname{spam})+p(\mathcal{D} | \neg \operatorname{spam}) p(\neg \operatorname{spam})</script><p>We use exactly the same procedure to compute $p(D|\neg spam)$ as we did to compute the $p(D|spam)$. </p><p>By calculating the posterior probabilities, we can get the approximate prediction on whether the email is spam or not.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Bishop PRML chapter 1.5.4</li><li>Shuogh blog: <a href="https://shuogh.github.io/2019/05/25/Discriminative-and-Generative-Models/" target="_blank" rel="noopener">Discriminative and generative models</a></li><li>AML(comp3008) 2012-2013 exam paper </li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Discriminative and Generative Models</title>
      <link href="/2019/05/25/Discriminative-and-Generative-Models/"/>
      <url>/2019/05/25/Discriminative-and-Generative-Models/</url>
      
        <content type="html"><![CDATA[<p>The classification problem can be broken down into two seperate stages: </p><ul><li>The <em>inference</em> stage: train data to learn a model for $p(C_k|x)$</li><li>The <em>decision</em> stage: use these posterior probabilities to make optimal class assignments </li></ul><p>To solve the classification, there are actually <strong>three</strong> distinct approaches.</p><h2 id="Generative-Models"><a href="#Generative-Models" class="headerlink" title="Generative Models"></a>Generative Models</h2><p>To solve the <em>inference</em> problem, we should determine the class-conditional densities $p(x|C_k)$ for each class $C_k$ individually. Also infer the prior class probabilities $p(C_k)$. Then use Bayes’ theorem in the form</p><script type="math/tex; mode=display">p\left(\mathcal{C}_{k} | \mathbf{x}\right)=\frac{p\left(\mathbf{x} | \mathcal{C}_{k}\right) p\left(\mathcal{C}_{k}\right)}{p(\mathbf{x})}</script><p>to find the posterior class probabilities $p(C_k|x)$. </p><p>For the denominator, it can be calculated by <script type="math/tex">p(\mathbf{x})=\sum_{k} p\left(\mathbf{x} | \mathcal{C}_{k}\right) p\left(\mathcal{C}_{k}\right)</script>.</p><p>Equivalently, the joint distribution $p(x,C_k)$ can also be modelled directly and then normalize to obtain the posterior probabilities. </p><p>Given the posterior probabilities, we use decision theory to determine class membership for each input $x$. This kind of method is called <em>generative models</em>, which model the distribution of inputs as well as the outputs. The name “<em>generative</em>“ is because by sampling from them it is possible to generate synthetic data points in the input space.</p><p>The <strong>examples</strong> of generative models:</p><ul><li>Naive Bayes, Latent Dirichlet allocation, Gaussian Process…</li></ul><h2 id="Discriminative-Models"><a href="#Discriminative-Models" class="headerlink" title="Discriminative Models"></a>Discriminative Models</h2><p>Solve the inference problem of determining the posterior class probabilities $p(C_k|x)$, and then make prediction using decision theory. </p><p>The methods which model the posterior probabilities $p(C_k|x)$ <strong><em>directly</em></strong> are called <em>discriminative models</em>. </p><p><strong>or </strong></p><p>Find a function $f(x)$, called a discriminant function, which maps each input $x$ directly onto a class label. </p><p><strong>Examples</strong> of discriminative models:</p><ul><li>kNN, perceptron, decision tree, linear regression, logistics regression, SVM, neural network…</li></ul><h2 id="The-Merits-of-Each-Method"><a href="#The-Merits-of-Each-Method" class="headerlink" title="The Merits of Each Method"></a>The Merits of Each Method</h2><p><strong><em>Generative models</em></strong> are most demanding, since it involve finding the joint distribution over both $x$ and $C_k$. For many application, $X$ have high dimensionality and consequently we may need a large training set in order to be able to determine the <strong>class-conditional densities</strong> （类条件概率密度，就是后验概率，我们的目标）to reasonable accuracy.<br>One distinctive use case of the generative models is <em>outlier detection</em> （离群点检测）. The margin density of data $p(x)$ can be determined using the formula menetioned above. It is usefule for detecting new data points that have low probability under the model and for which the predictions may be of low accuracy, which is know as <em>outlier detection</em> and <em>novelty detection</em>.</p><p><strong><em>Discriminative approaches</em></strong> is simpler. The second approach can obtain the posterior probabilities $p(C_k|x)$ directly from the data points. The thrid approach is much simpler, in which we use the training data to find a discriminant function $f(x)$ that maps each $x$ directly onto a class label (It combine the inference and decision stages into a single learning problem). However, in the third method, we no loner have access to posterior probabilities.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Bishop PRML Chapter 1.5.4</li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Mining Information Theory</title>
      <link href="/2019/05/25/Data-Mining-Information-Theory/"/>
      <url>/2019/05/25/Data-Mining-Information-Theory/</url>
      
        <content type="html"><![CDATA[<h1 id="Information-Theory-and-Feature-Selection"><a href="#Information-Theory-and-Feature-Selection" class="headerlink" title="Information Theory and Feature Selection"></a>Information Theory and Feature Selection</h1><ul><li>Outline:<ul><li>Information</li><li>Entropy</li><li>Mutual information</li><li>for feature selection</li></ul></li></ul><h2 id="Information"><a href="#Information" class="headerlink" title="Information"></a>Information</h2><p>Information, also can be seen as <strong>uncertainty</strong> and <strong>surprise</strong>.</p><p>$I=-log_2{p(x)}$ Since the $p(x)$ is the probability of event $x$, the value $&lt;1$.</p><p><strong>Shannon entropy:</strong></p><script type="math/tex; mode=display">H(p)=-\sum_{x} p(x) \log _{2} p(x)</script><p>( entropy = the probability of an event * information of this event )</p><p>Shannon entropy is the measure of uncertainty. </p><p>(香农熵描述的是混乱程度，而且information这个概念其实也是从这个角度给出的，不确定性越大，这个事件携带的信息越多。)</p><h2 id="K-L-Divergence"><a href="#K-L-Divergence" class="headerlink" title="K-L Divergence"></a>K-L Divergence</h2><p>Two probability distribution $f(x)$ and $g(x)$, the K-L divergence is :</p><script type="math/tex; mode=display">D(f \| g)=\sum_{x \in X} f(x) \log \frac{f(x)}{g(x)}</script><ul><li>Compare the entropy of two distribution over the same random variable</li><li>Heuristically: number of additional bits encoding a random variable with distribution $f(x)$ using $g(x)$.</li></ul><p>It can be seen as the $D(f|g) =\sum_{x \in X} [-f(x)log_2 g(x)+f(x)log_2f(x)] $, the first term is to encode $f(x)$ using the the encoding method of $g(x)$.Therefore, it can be seen as the distance between two encoding function ( or distribution). </p><p>When minimizing K-L against a fixed reference distribution $p$, the task is euivalent to minimizing <strong>cross entropies</strong>. It can be written as: $D(f|g) =\sum_{x \in X}f(x)log_2f(x) - \sum_{x \in X}f(x)log_2 g(x) $</p><p>The second term is what we use in the cross entropy loss function.</p><p>The form of cross entropy:</p><script type="math/tex; mode=display">H(p, q)=-\sum_{x} p(x) \log _{2} q(x)</script><p><strong>Note</strong>:<br>The K-L divergence can not be used as a measure for the distance between $f$ and $g$, since it is not symmetric, $D(f | g)$ is not equal to $D(g | y)$. </p><h2 id="Conditional-Entropy"><a href="#Conditional-Entropy" class="headerlink" title="Conditional Entropy"></a>Conditional Entropy</h2><p>The $I$ is <strong>realized information</strong>, which is the difference between the entropy of $H(C)$ and the contional entropy $H(C|X=x)$. And the realized information is defined as:</p><script type="math/tex; mode=display">I[C ; X=x]=H(C)-H(C | X=x)</script><p>Given the observation of $X$, the entropy of $C$ is decrease, which is written as $H(C | X=x)$.</p><p>The realized information is not necessarily positive. If it is negative, the entropy will increase.</p><p>Form of the contional entropy (from <strong>PRML</strong>): $H(Y | X)=-\sum_{x_{i}, y_{j}}^{m, n} p\left(x_{i}, y_{j}\right) \cdot \log _{2} p\left(y_{j} | x_{i}\right)$ </p><h2 id="Mutual-Information"><a href="#Mutual-Information" class="headerlink" title="Mutual Information"></a>Mutual Information</h2><p>Mutual information is the expected information a <strong>feature</strong> gives us about a classs:</p><p>$I[C ; X]=H(C)-\sum \operatorname{Pr}(X=x) H(C | X=x)$</p><p>Note:</p><ul><li>Mutual information is always positive.</li><li>Is only 0 when the X and C are statistically independent.</li><li>Is symmetric in X and C</li></ul><p><strong>Example of calculating the mutual information</strong>:</p><div class="table-container"><table><thead><tr><th></th><th style="text-align:center">Indicator X</th><th></th></tr></thead><tbody><tr><td>Class $C$</td><td style="text-align:center">“Paint”</td><td>“Not Paint”</td></tr><tr><td>Art</td><td style="text-align:center">12</td><td>45</td></tr><tr><td>Music</td><td style="text-align:center"></td><td>45</td></tr></tbody></table></div><p>The entropy of C: $H(C)=57/102 \cdot log_2(57/102)+ 45/102\cdot log_2(45/102)=0.99$</p><p>$H[C|X=”paint”]=0$ ,since the “paint” can be certain that the story is about art.</p><p>$H[C|X=”not paint”]=1.0$, which we can calculate from the distribution.</p><p>$I[C;X]=H[C]-Pr(x=1)H[C|X=1]-Pr(X=0)H[C|X=0]$ = 0.99-12/102<em>0-90/102 </em>1 =0.11 </p><p>Therefore, the mutual information is 0.11, which is the expected reduction in uncertainly.</p><p><strong>Note</strong>:</p><p>In the decision tree, mutual information is used as <strong>information gain</strong>. The information gain is the strategy used to choose the best feature for decision. See the <a href="https://www.zhihu.com/question/39436574" target="_blank" rel="noopener">zhihu</a> for more detail.</p><p>And this is the way which most people use to find the informative features.</p><h2 id="Joint-and-Conditional-Entropy"><a href="#Joint-and-Conditional-Entropy" class="headerlink" title="Joint and Conditional Entropy"></a>Joint and Conditional Entropy</h2><p>$H[X, Y]=-\sum_{x, y} \operatorname{Pr}(X=x, Y=y) \log _{2} \operatorname{Pr}(X=x, Y=y)$</p><p>Kind of the joint distribution. </p><p>Using this, conditional mutual information can be derivated:</p><p>$I[C ; Y | X]=H[C | X]-H[C | Y, X]$</p><ul><li>we ask how much information does Y contain about C if we “control” for X.</li></ul><h2 id="Interaction"><a href="#Interaction" class="headerlink" title="Interaction"></a>Interaction</h2><p>Contional mutual information $I [C ; Y | X]$ is positive:</p><ul><li><p>But might be smaller/larger/equal to $I[C;Y]$</p></li><li><p>If $I[C;Y|X]=0$: C and Y are conditionally independent given X; Otherwise there is an interaction between X and Y(regarding their information about C)</p></li><li><p>$I[C;Y|X]&lt;I[C;Y]$: Some of the information in Y about C is redundant given X</p></li><li><p>Use this to define the <strong>interaction information</strong>: $I(C;Y;X)=I(C;Y|X)-I(C;Y)$</p><p>(Actually not very familiar with this interaction)</p></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>CAML机器学习系列2：<a href="https://www.cnblogs.com/maybe2030/p/5514841.html" target="_blank" rel="noopener">深入浅出ML之Entropy-Based家族</a></li><li>The slide from Markus: <a href="https://www.southampton.ac.uk/~mb1a10/stats/Information.pdf" target="_blank" rel="noopener">information</a> </li><li>Bishop PRML</li></ol>]]></content>
      
      
      <categories>
          
          <category> Data Mining </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python Init Modules</title>
      <link href="/2019/05/14/Python-Init-Modules/"/>
      <url>/2019/05/14/Python-Init-Modules/</url>
      
        <content type="html"><![CDATA[<p>__init__.py 文件的作用是将文件夹变为一个Python模块,Python 中的每个模块的包中，都有__init__.py 文件。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Python __init__.py <a href="https://www.cnblogs.com/Lands-ljk/p/5880483.html" target="_blank" rel="noopener">作用详解</a> </li></ol>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning Batch Normalization</title>
      <link href="/2019/05/13/Deep-Learning-Batch-Normalization/"/>
      <url>/2019/05/13/Deep-Learning-Batch-Normalization/</url>
      
        <content type="html"><![CDATA[<h1 id="Why-we-need-batch-normalization-in-neural-network"><a href="#Why-we-need-batch-normalization-in-neural-network" class="headerlink" title="Why we need batch normalization in neural network?"></a>Why we need batch normalization in neural network?</h1><h2 id><a href="#" class="headerlink" title=" "></a> </h2><p>It can help the neural network to converge more quickly.</p><p>Make the different features into the same scale, get rid of the influence of different scale.</p><p>防止梯度爆炸和梯度消失</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>zhihu: <a href="https://www.zhihu.com/question/281816000" target="_blank" rel="noopener">神经网络中的归一化除了减少计算量，还有什么作用？</a></li><li>towards data science: <a href="https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c" target="_blank" rel="noopener">Batch normalization in Neural Networks</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning RNN sequence model</title>
      <link href="/2019/05/09/Deep-Learning-RNN-sequence-model/"/>
      <url>/2019/05/09/Deep-Learning-RNN-sequence-model/</url>
      
        <content type="html"><![CDATA[<p>Take down the note when I came accross the bugs during doing the lab 7.3. </p><p>The key words:<br><code>pack padded sequence</code>, <code>pad packed sequence</code>, the output of <code>lstm</code> model.</p><p>The code is listed below:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImprovedRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, embedding_dim, hidden_dim, output_dim)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        </span><br><span class="line">        self.embedding = nn.Embedding(input_dim, embedding_dim)</span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        self.lstm=nn.LSTM(embedding_dim,hidden_dim,batch_first=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#         raise NotImplementedError()</span></span><br><span class="line">        self.fc = nn.Linear(hidden_dim, output_dim)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text_len)</span>:</span></span><br><span class="line">        text, lengths = text_len</span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line"><span class="comment">#         print(embedded.data.size())  # (sentence length, batch_size,hidden_dim)</span></span><br><span class="line"><span class="comment">#         print(lengths)           # the tensor(size 64), contains the length of sizes</span></span><br><span class="line">        embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths)</span><br><span class="line"><span class="comment">#         print(embedded.data.size()) # the packed_sequence record the series data and the tensor recording each length </span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line"><span class="comment">#         print(embedded[0].size())</span></span><br><span class="line">        _,(last_state,_)=self.lstm(embedded)</span><br><span class="line"><span class="comment">#         lstm_out_pad,length_sentence=nn.utils.rnn.pad_packed_sequence(lstm_out)</span></span><br><span class="line">        print(last_state.size())</span><br><span class="line"><span class="comment">#         lstm_final_out=lstm_out_pad[length_sentence-1]  # just use the final timestep output</span></span><br><span class="line"><span class="comment">#       lstm_out_pad [0] is the data and [1] records the length of each sentence</span></span><br><span class="line"><span class="comment">#         print("...",lstm_out.data.size(),"\n ssss",type(lstm_out.data))</span></span><br><span class="line"><span class="comment">#         print("\n haha...",lstm_out_pad)</span></span><br><span class="line"><span class="comment">#         print(lstm_out_pad[0][-1][63])</span></span><br><span class="line"><span class="comment">#         length_63=lstm_out_pad[-1][63]  # Use [-1] can't get real last one </span></span><br><span class="line"><span class="comment">#         print("I'm the length of first sentence:",length_63)</span></span><br><span class="line"><span class="comment">#         print("i'm the data:",lstm_out_pad[0][length_63-1],lstm_out_pad[0][length_63 - 1][63])</span></span><br><span class="line"><span class="comment">#         print("im the length list:",lstm_out_pad[-1])</span></span><br><span class="line"><span class="comment">#         print(lstm_final_out.size())</span></span><br><span class="line">        out=self.fc(lstm_final_out)</span><br><span class="line"><span class="comment">#         print("emm heng?")</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"><span class="comment">#         raise NotImplementedError()</span></span><br><span class="line">        </span><br><span class="line">INPUT_DIM = len(TEXT.vocab)  <span class="comment"># 25002</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">50</span></span><br><span class="line">HIDDEN_DIM = <span class="number">100</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">imodel = ImprovedRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Train and evaluate the model</span></span><br><span class="line"><span class="comment"># YOUR CODE HERE</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">torchbearer_trial = Trial(imodel, optimizer, criterion, metrics=[<span class="string">'acc'</span>, <span class="string">'loss'</span>]).to(device)</span><br><span class="line">torchbearer_trial.with_generators(train_generator=MyIter(train_iterator), val_generator=MyIter(valid_iterator), test_generator=MyIter(test_iterator))</span><br><span class="line"><span class="comment"># torchbearer_trial.with_train_generator(MyIter(train_iterator))</span></span><br><span class="line">torchbearer_trial.run(epochs=<span class="number">5</span>)</span><br><span class="line">torchbearer_trial.predict()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux Server set thread for Pytorch</title>
      <link href="/2019/05/07/Linux-Server-set-thread-for-Pytorch/"/>
      <url>/2019/05/07/Linux-Server-set-thread-for-Pytorch/</url>
      
        <content type="html"><![CDATA[<p>During the time of doing my course work for Advanced Machine Learning, I ran my deep learning scripts on the server. In the first time, I saw the %CPU of my job was always very high. (And then, the admin killed my job since it stuck other jobs…sorry, i didn’t know this at that time)</p><p>To avoid the effect on other users, before runing our Pytorch scripts, we should run the following code in the beginning to set the thread. </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">OMP_NUM_THREADS=1</span><br><span class="line"><span class="built_in">export</span> OMP_NUM_THREADS</span><br></pre></td></tr></table></figure><p>This allow us to use only one thread in our one job.</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git Rebase to keep commit log clean</title>
      <link href="/2019/05/05/Git-Rebase-to-keep-commit-log-clean/"/>
      <url>/2019/05/05/Git-Rebase-to-keep-commit-log-clean/</url>
      
        <content type="html"><![CDATA[<p>There always be the cases that we are developing a new feature on seperate branch when we are using <code>Git</code>. There are many commit log like “fix type”, “correct the error” etc. When we merge the branch to <code>master</code> branch, we don’t want these stupid commit log appear in the commit log of <code>master</code> branch. </p><p>To merge <code>development</code> branch to <code>master</code> branch:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git checkout master</span><br><span class="line">git merge development</span><br></pre></td></tr></table></figure></p><p>If we want to make our commit log clean, then you should use <code>rebase</code>.</p><h2 id="Rebase"><a href="#Rebase" class="headerlink" title="Rebase"></a>Rebase</h2><p>example:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始开发一个新 feature</span></span><br><span class="line">$ git checkout -b new-feature master</span><br><span class="line"><span class="comment"># 改了一些代码</span></span><br><span class="line">$ git commit -a -m <span class="string">"Start developing a feature"</span></span><br><span class="line"><span class="comment"># 刚刚的修改有点问题，再改一下</span></span><br><span class="line">$ git commit -a -m <span class="string">"Fix something from the previous commit"</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 紧急修复，直接在 master 分支上改点东西</span></span><br><span class="line">$ git checkout master</span><br><span class="line"><span class="comment"># 改了一些代码</span></span><br><span class="line">$ git commit -a -m <span class="string">"Fix security hole"</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 开始交互式地 rebase 了</span></span><br><span class="line">$ git checkout new-feature</span><br><span class="line">$ git rebase -i master</span><br></pre></td></tr></table></figure></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Git tips: <a href="https://www.lovelucy.info/git-tips-combine-commits-keep-your-branch-clean.html" target="_blank" rel="noopener">合并 commit 保持分支干净整洁</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux Run Script even after logging out</title>
      <link href="/2019/05/02/Linux-Run-Script-even-after-logging-out/"/>
      <url>/2019/05/02/Linux-Run-Script-even-after-logging-out/</url>
      
        <content type="html"><![CDATA[<h2 id="Nohup"><a href="#Nohup" class="headerlink" title="Nohup"></a>Nohup</h2><ul><li>image_haha</li></ul><h2 id="Rerference"><a href="#Rerference" class="headerlink" title="Rerference"></a>Rerference</h2><ol><li>linux后台执行命令：<a href="https://blog.csdn.net/liuyanfeier/article/details/62422742" target="_blank" rel="noopener">&amp;和nohup</a></li><li>Nohup Command in Linux: <a href="https://linuxhint.com/nohup_command_linux/" target="_blank" rel="noopener">Linux Hint</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VAE Variational Autoencoder</title>
      <link href="/2019/05/01/VAE-Variational-Autoencoder/"/>
      <url>/2019/05/01/VAE-Variational-Autoencoder/</url>
      
        <content type="html"><![CDATA[<p>差分自编码器，跟普通的自编码器不同，有着他自己特殊的地方。</p><p>通过编码器学习图像的编码，得到其潜在表征向量（这里学习其作为高斯分布的参数）。</p><p>为了训练encoder和decoder，loss function由两部分组成：</p><ul><li>KL divergence来表示隐含向量与标准正态分布之间差异的loss</li><li>另外一个loss使用生成图片与原图片的均方误差来表示</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>部分公式推导 <a href="http://skyhigh233.com/blog/2018/04/05/vae/" target="_blank" rel="noopener">KL divergence</a></li><li>Github <a href="https://github.com/pytorch/examples/blob/master/vae/main.py" target="_blank" rel="noopener">example code</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Next() in Python</title>
      <link href="/2019/04/30/Next-in-Python/"/>
      <url>/2019/04/30/Next-in-Python/</url>
      
        <content type="html"><![CDATA[<p>To fetch a item from generator, <code>next()</code> can be used: <code>Return the next item from the iterator.</code></p><p>If a variable is not a generator, <code>next()</code> can be used along with <code>iter()</code>.</p><p><strong>Python code snippet</strong>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">next(a)</span><br><span class="line"><span class="comment"># output: TypeError: 'list' object is not an iterator</span></span><br><span class="line"></span><br><span class="line">b=iter(a)</span><br><span class="line">next(b)</span><br><span class="line"><span class="comment"># output: 1</span></span><br><span class="line">next(b)</span><br><span class="line"><span class="comment">#  output: 2</span></span><br><span class="line">next(b)</span><br><span class="line"><span class="comment">#  output: 3</span></span><br><span class="line">next(b)</span><br><span class="line"><span class="comment">#  StopIteration</span></span><br></pre></td></tr></table></figure></p><p>My case:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dataset construction</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(), <span class="comment"># convert to tensor</span></span><br><span class="line">    transforms.Lambda(<span class="keyword">lambda</span> x: x.view(image_dim)) <span class="comment"># flatten into vector</span></span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">train_set = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">'./data/FashionMNIST'</span></span><br><span class="line">    ,train=<span class="literal">True</span></span><br><span class="line">    ,download=<span class="literal">True</span></span><br><span class="line">    ,transform=transform</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">    train_set, batch_size=batch_size</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fetch images by next() function</span></span><br><span class="line"><span class="comment"># Since the obj returned by DataLoader was not iterator, I also used iter()</span></span><br><span class="line">images = next(iter(train_loader))</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sklearn Split Train and Test</title>
      <link href="/2019/04/30/Sklearn-Split-Train-and-Test/"/>
      <url>/2019/04/30/Sklearn-Split-Train-and-Test/</url>
      
        <content type="html"><![CDATA[<p>There are several ways to split the data set into training data set and test data set.</p><p>In this blog, I will talk about the difference between these approaches.</p><h2 id="sklearn-model-selection-train-test-split"><a href="#sklearn-model-selection-train-test-split" class="headerlink" title="sklearn.model_selection.train_test_split"></a>sklearn.model_selection.train_test_split</h2><p>Doc: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html</a></p><h2 id="sklearn-model-selection-ShuffleSplit"><a href="#sklearn-model-selection-ShuffleSplit" class="headerlink" title="sklearn.model_selection.ShuffleSplit"></a>sklearn.model_selection.ShuffleSplit</h2><p>Doc: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html" target="_blank" rel="noopener">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Sklearn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Better style for Python Programming</title>
      <link href="/2019/04/29/Better-style-for-Python-Programming/"/>
      <url>/2019/04/29/Better-style-for-Python-Programming/</url>
      
        <content type="html"><![CDATA[<p>How to write better code with good style</p><p>Here is a guide.</p><p>机器之心：<a href="https://www.jiqizhixin.com/articles/2019-04-29-5" target="_blank" rel="noopener">PyTorch最佳实践，怎样才能写出一手风格优美的代码</a></p><h2 id="Python-Name-Convention"><a href="#Python-Name-Convention" class="headerlink" title="Python Name Convention"></a>Python Name Convention</h2><p>python 命名规范</p><div class="table-container"><table><thead><tr><th style="text-align:center">Type</th><th style="text-align:center">Convention</th><th style="text-align:center">Example</th></tr></thead><tbody><tr><td style="text-align:center">Packages &amp; Modules</td><td style="text-align:center">lower_with_under</td><td style="text-align:center">from <strong>prefetch_generator</strong> import BackgroundGenerator</td></tr><tr><td style="text-align:center">Classes</td><td style="text-align:center">CapWords</td><td style="text-align:center">class <strong>Dataloader</strong></td></tr><tr><td style="text-align:center">Constants</td><td style="text-align:center">CAPS_WITH_UNDER</td><td style="text-align:center"><strong>BATCH_SIZE</strong>=16</td></tr><tr><td style="text-align:center">Instances</td><td style="text-align:center">lower_with_under</td><td style="text-align:center"><strong>dataset</strong> = Dataset</td></tr><tr><td style="text-align:center">Methods &amp; Functions</td><td style="text-align:center">lower_with_under()</td><td style="text-align:center">def <strong>visualize_tensor</strong>()</td></tr><tr><td style="text-align:center">Variables</td><td style="text-align:center">lower_with_under</td><td style="text-align:center"><strong>background_colour</strong> = ‘Blue’</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch CUDA experience</title>
      <link href="/2019/04/29/Pytorch-CUDA-experience/"/>
      <url>/2019/04/29/Pytorch-CUDA-experience/</url>
      
        <content type="html"><![CDATA[<p>In my experience of using the lab server to train my model, I met the problem of OOM(out of memory). Here I attach some solution and thinking in the following article.</p><p>Assume such scenario:</p><ul><li>The default CUDA is full and even you want to do <code>torch.tensor([1,2,3]).cuda()</code> you will get <strong>OOM</strong> error.</li></ul><p>You shoul trying to choose another GPU.</p><h2 id="CUDA-VISIBLE-DEVICES"><a href="#CUDA-VISIBLE-DEVICES" class="headerlink" title="CUDA_VISIBLE_DEVICES"></a>CUDA_VISIBLE_DEVICES</h2><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">os.environ[&apos;CUDA_VISIBLE_DEVICES&apos;] = &apos;2,3&apos;</span><br></pre></td></tr></table></figure><p>Add this piece of code into your <strong>script</strong> file, and when your execute your code, you will use the corrsponding GPU.<br>(<strong>Note</strong>: This will not be useful in <strong>Jupyter</strong> Notebook.)</p><p>OR</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=2 python test.py</span><br></pre></td></tr></table></figure><p>When you execute your script file, add the <code>CUDA_VISIBLE_DEVICES=2</code> in the begining. Then the script will run on the certain GPU.</p><h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><p>Even you set your GPU of 2 or 3 using this way, in the output, the device will show <code>tensor([1, 2, 3], device=&#39;cuda:0&#39;)</code>. </p><ul><li>From pytorch forum of <a href="https://discuss.pytorch.org/t/cuda-visible-device-is-of-no-use/10018/8" target="_blank" rel="noopener">@pjavia ‘s answer</a>:<br>  <code>@MrTuo This is how pytorch 0.4.1 convention works. If you say CUDA_VISIBLE_DEVICES=2, 3. Then for pytorch GPU - 2 is cuda:0 and GPU - 3 is cuda:1. Just check your code is consistent with this convention or not?</code></li></ul><p>And I tested on the torch 1.0.1, it seems also consistent with this answer.</p><h2 id="Torch-cuda"><a href="#Torch-cuda" class="headerlink" title="Torch.cuda"></a>Torch.cuda</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.set_device(1)</span><br><span class="line"></span><br><span class="line">torch.tensor([1,2,3]).cuda()</span><br><span class="line"># output: tensor([1, 2, 3], device=&apos;cuda:1&apos;)</span><br></pre></td></tr></table></figure><p>This code of first line is useful on <strong>Jupyter</strong> Notebook. When you set certain GPU device, the following code will use this GPU.</p><p>It’s kind of set the GPU environment.</p><h2 id="Torch-device"><a href="#Torch-device" class="headerlink" title="Torch.device"></a>Torch.device</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&apos;cuda:3&apos;)</span><br><span class="line"># X = X.to(device)</span><br></pre></td></tr></table></figure><p>Set a device of certain GPU, when you are executing the code, transfer the variable into the device(it can also be <code>CPU</code>).</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://jdhao.github.io/2018/04/02/pytorch-gpu-usage/" target="_blank" rel="noopener">Set Default GPU in PyTorch</a></li><li>Pytorch forum: <a href="https://discuss.pytorch.org/t/cuda-visible-device-is-of-no-use/10018/8" target="_blank" rel="noopener">CUDA_VISIBLE_DEVICE is of no use </a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux Commands when using Server</title>
      <link href="/2019/04/28/Linux-Commands-when-using-Server/"/>
      <url>/2019/04/28/Linux-Commands-when-using-Server/</url>
      
        <content type="html"><![CDATA[<p>When I am using the lab server, there are some commands that I need to use to see the situation of server.</p><h2 id="top"><a href="#top" class="headerlink" title="top"></a>top</h2><p>作用等同于任务管理器</p><p>You can see the CPU, Memory situation by using this command.</p><h2 id="nvidia-smi"><a href="#nvidia-smi" class="headerlink" title="nvidia-smi"></a>nvidia-smi</h2><p>See the GPU situation, the GPU memory and the some other things.</p><h2 id="ps"><a href="#ps" class="headerlink" title="ps"></a>ps</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -u [username]</span><br></pre></td></tr></table></figure><p>To see the jobs of one user in this server.</p><h2 id="echo"><a href="#echo" class="headerlink" title="echo"></a>echo</h2><p>To see the current working dir path:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo $PWD</span><br></pre></td></tr></table></figure></p><h2 id="Rerference"><a href="#Rerference" class="headerlink" title="Rerference"></a>Rerference</h2><ol><li>每天一个linux命令（44）：<a href="https://www.cnblogs.com/peida/archive/2012/12/24/2831353.html" target="_blank" rel="noopener">top命令</a></li><li>CUDA之nvidia-smi命令 <a href="https://blog.csdn.net/Bruce_0712/article/details/63683787" target="_blank" rel="noopener">详解</a> </li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux Server virtual env</title>
      <link href="/2019/04/28/Linux-Server-virtual-env/"/>
      <url>/2019/04/28/Linux-Server-virtual-env/</url>
      
        <content type="html"><![CDATA[<p><strong>How to create the virtual env in the server of lab.</strong></p><p>In my group coursework for the advanced ML, I want to run the code of the first solution in this competition. The requirments wasn’t satisfied in the server, so I want to create virtual env to built such environment. This blog records the process of building envirment to run deep learning task.</p><h2 id="virtualenv"><a href="#virtualenv" class="headerlink" title="virtualenv"></a>virtualenv</h2><p>If your linux server already has the <code>virtualenv</code> module, you can use virtual env to create virtual environment. You can check it using <code>pip list</code>.</p><p>In my try, I tried to install the <code>virtualenv</code> in the beginning. I found that the permmission is denied, since I don’t have the root access on this server supplied by teachers.</p><p>I found the reason and solution in this <a href="https://github.com/googlesamples/assistant-sdk-python/issues/236" target="_blank" rel="noopener">issue</a>. Therefore, I did another try which is the following one.</p><h2 id="Python-m-venv"><a href="#Python-m-venv" class="headerlink" title="Python -m venv"></a>Python -m venv</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python3 -m venv env</span><br><span class="line">source ./env/bin/activate</span><br></pre></td></tr></table></figure><p>This solution just needs you to have the Python in your system (any Linux has the Python within system). In this way, I can activate the virual environment and <code>pip install</code> the specific modules.</p><h2 id="我的解决方案"><a href="#我的解决方案" class="headerlink" title="我的解决方案"></a>我的解决方案</h2><p>目标：python3.6 </p><p>过程：    </p><ol><li>python -m env</li><li>在python创建出来的虚拟环境中安装virtualenv</li><li>通过virtualenv创建对应python3.6版本的虚拟环境</li></ol><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>an issue on Github: <a href="https://github.com/googlesamples/assistant-sdk-python/issues/236" target="_blank" rel="noopener">Permission denied</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kaggle Competition: Humpback Whale Identification</title>
      <link href="/2019/04/27/Kaggle-Competition-Humpback-Whale-Identification/"/>
      <url>/2019/04/27/Kaggle-Competition-Humpback-Whale-Identification/</url>
      
        <content type="html"><![CDATA[<p>Kaggle竞赛第一名方案解读</p><h2 id="Description-of-Competition"><a href="#Description-of-Competition" class="headerlink" title="Description of Competition"></a>Description of Competition</h2><p>目的：构建算法识别鲸鱼个体</p><p>难点：</p><ul><li>训练样本的严重不均衡</li><li>存在接近三分之一的无标注（new whale）数据</li></ul><p>Some new terminology:</p><ul><li>Few-shot learning: <a href="https://blog.csdn.net/xhw205/article/details/79491649" target="_blank" rel="noopener">what’s few shot learning</a></li><li>细粒度分类: that’s why we need mask. <a href="https://www.imooc.com/article/30053" target="_blank" rel="noopener">mask-CNN</a>,<a href="https://cloud.tencent.com/developer/article/1010499" target="_blank" rel="noopener">什么是mask</a></li><li>triplet loss: ？？？</li><li>SE-resneXt154: 一个新的分类模型</li><li>伪标签：？？</li></ul><hr><p><strong>Pipeline：</strong></p><h2 id="Input-of-the-models"><a href="#Input-of-the-models" class="headerlink" title="Input of the models"></a>Input of the models</h2><ul><li>RGB+mask</li><li>Data Augmentation:<ul><li>有人提出鲸鱼尾部不对称，翻转之后是新的类别</li></ul></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>kaggle competition: <a href="https://www.kaggle.com/c/humpback-whale-identification" target="_blank" rel="noopener">Humpback Whale Identification</a></li><li>机器之心: <a href="https://zhuanlan.zhihu.com/p/58496385" target="_blank" rel="noopener">Kaggle第一名竞赛方案解读</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Kaggle </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> Kaggle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Interview-1 QuantumBlack Data Science Intern</title>
      <link href="/2019/04/15/Interview-1-QuantumBlack-Experience/"/>
      <url>/2019/04/15/Interview-1-QuantumBlack-Experience/</url>
      
        <content type="html"><![CDATA[<p>记第一次在英国公司面试</p><h2 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h2><p>第一次面试，quantum black这个公司，面试官是两个小姐姐。公司整体人很好，刚进门的时候有小哥路过还打招呼问我，后来在餐厅等候的时候还有小哥问我吃不吃巧克力，公司整体氛围相当不错。面试官也特别友善。</p><p>面试开始先有一个简单的introduction，让面试官认识你。没有准备好这个brief introduction。</p><p>面试过程表现不太好。。。感觉一是因为英语不够熟练，刚开始不太能get到小姐姐的问题，表现不好。二是后面technical方面的问题的时候，我忘了一些模型的细节，后面详细写。</p><h2 id="case-part"><a href="#case-part" class="headerlink" title="case part"></a>case part</h2><p>第一部分是实际案例部分，案例问题是关于fraud detection。给定一组很大的银行的交易数据，such as 100million条，其中200条是诈骗交易，我们要进行诈骗交易的检测。</p><p>第一步让我构建feature，觉得能从之前的数据中构建出来什么feature。最开始没太理解，后来也没答好，确实不知道能构建什么feature</p><p>第二步让我建立模型来解决这个问题。我提出使用逻辑回归模型来进行预测。接着会来一连串的问题，为什么会选用逻辑回归来预测？ 我要如何训练和测试这个模型？</p><ul><li>关于loss function这一块，问我如何构建。 我说使用交叉熵，但是我忘了交叉熵的公式了。。。。</li><li>metric to evaluate 这个模型，我说可以使用confusion matrix。要求来画出混淆矩阵，紧张了一下没画出来，后来画出来了。问到了precision，recall和f值。结合案例又问了问题，问我应该重点关注哪个值，这里回答不好。。。</li><li>如何split training data set。这个数据严重不平衡，如何做。。。。我也不知道回答的好还是不好。。先说80 20 split，后来说可以使用cross validation来进行交叉检验。小姐姐针对这个问题提出疑问，可能有的fold没有fraud point。。。</li></ul><p>这部分整体感觉，有点崩，也有些超时。感觉这里应该有自己的独立思考，根据相应的案例进行变通，应该是要跟面试官进行discuss的，我没做好心理准备，导致被面试官牵着走，效果也不好。模型不应该一成不变，应该根据相应的case有不同变通。</p><h2 id="technical-part"><a href="#technical-part" class="headerlink" title="technical part"></a>technical part</h2><p>这一块不是case，是要问理论的部分。这一块刚开始其实还是比较自信的，因为自我感觉理论掌握的还不错。</p><ol><li><p>ROC curve<br>之前看过ROC curve，但是这次死活也想不起来。。。难受，这个东西业界用的比较多，之前看过一次，但是这次之前忘了看，实属失误</p></li><li><p>逻辑回归<br>这里小姐姐结合线性回归和逻辑回归来问我问题。 还好前几天看官网案例的时候看到逻辑回归用的比较多，提前准备了一下。这里主要看线性回归和逻辑回归的理解。分别问到了线性回归的方程表现形式，loss function是什么，梯度下降又是什么（这里画图来描述），如何使用梯度下降进行优化（不必要推导导数公式）。</p></li></ol><p>接着问了逻辑回归和线性回归之间的联系。时间限制，我写了一个公式，小姐姐知道了我的意思就开始下一个问题了。</p><ol><li><p>其他非线性分类的模型<br>我回答了 SVM，接着让我描述SVM和他的原理。我说svm基本状态是线性分类的，要做到非线性要用kernel。接着让我描述kernel，kernel是什么。这一块花的时间比较多，我有时候没有搞明白她的意思。其实kernel我也没有办法说的很清楚，这一块是个失误。</p></li><li><p>tree - ensemble<br>还好我提前也准备了这一块，集成学习这一块。可惜boosting部分忘记了细节，太紧张了没回答上来。</p></li></ol><p>bagging，我描述了bagging的idea。小姐姐针对bagging问了我问题，这些models是一个model吗，是不是不同。</p><p>boosting，描述的没有很清楚。我没讲清楚如何训练互补的model。。。。。。给data set赋予权重，每个数据都有不同的权重（最开始没讲清楚），然后讲如何通过一个$\alpha$来变换之前之后的权重。（太紧张了，又没有提前准备，没回答好）</p><p>之后时间到了，结束。</p><h2 id="经验教训"><a href="#经验教训" class="headerlink" title="经验教训"></a>经验教训</h2><ol><li>准备好开头的小介绍  *重要</li><li>练习好英语，case discuss部分要灵活变通，表现自己的思考  ****重要</li><li>ROC curve  *重要，忘记准备了</li><li>SVM kernel *重要，学会讲这个东西</li><li>boosting  *重要，忘记准备，本身会</li><li>技术的问题都问的很详细，不会问你深度学习相关的东西，就只是问你base model的问题。准备时候要有侧重点，还好我提前看过了官网上的往期project，对知道他应该更多的问传统机器学习模型部分。但是一些具体的细节需要更加深入的理解，达到能给别人讲的程度。  ****重要 </li></ol>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Interview </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gaussian Mixed Model(GMM) and EM algorithm</title>
      <link href="/2019/04/14/Gaussian-Mixed-Model-GMM-and-EM-algorithm/"/>
      <url>/2019/04/14/Gaussian-Mixed-Model-GMM-and-EM-algorithm/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h2 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h2><h2 id="Mixed-Gaussian-Distribution"><a href="#Mixed-Gaussian-Distribution" class="headerlink" title="Mixed Gaussian Distribution"></a>Mixed Gaussian Distribution</h2><h2 id="Optimization-Method"><a href="#Optimization-Method" class="headerlink" title="Optimization Method"></a>Optimization Method</h2><h2 id="Reference："><a href="#Reference：" class="headerlink" title="Reference："></a>Reference：</h2><ol><li>知乎 <a href="https://zhuanlan.zhihu.com/p/30483076" target="_blank" rel="noopener">高斯混合模型(GMM)</a></li><li>知乎 <a href="https://zhuanlan.zhihu.com/p/31103654" target="_blank" rel="noopener">一文详解高斯混合模型原理</a></li><li>《统计学习方法》第九章 - EM算法及其推广——李航</li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Note-2 Feature Engineering</title>
      <link href="/2019/04/14/Note-2-Feature-Engineering/"/>
      <url>/2019/04/14/Note-2-Feature-Engineering/</url>
      
        <content type="html"><![CDATA[<img src="http://ww1.sinaimg.cn/large/637f3c58gw1exd7mcjk7yj28k33uwaoe.jpg" class="fe_img"><h2 id="What’s-Feature-Engineering"><a href="#What’s-Feature-Engineering" class="headerlink" title="What’s Feature Engineering"></a>What’s Feature Engineering</h2><p>In the application of machine learning or the field of data science, to achieve better performance on prediction or classification, we should not only choose the most suitable algorithm/model, but also we should use the suitable features.</p><p>Definition in wiki:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work.</span><br></pre></td></tr></table></figure></p><p>In a word, the feature engineering is to <strong>manually design what the input <code>x</code> should be and make our models work successfully</strong>.</p><h2 id="Importance"><a href="#Importance" class="headerlink" title="Importance"></a>Importance</h2><p>The features choice are important for our task.</p><ol><li><p>Better features make model have more flexibility.</p></li><li><p>Suitable features can use simple models</p></li><li><p>Achieve better performance </p></li></ol><h2 id="Sub-questions-of-Feature-Engineering"><a href="#Sub-questions-of-Feature-Engineering" class="headerlink" title="Sub-questions of Feature Engineering"></a>Sub-questions of Feature Engineering</h2><p>There are main three kinds of tasks in the feature engineering:</p><ol><li>Feature Construction<ul><li>Given a problem and raw data set, to construct the features using domain knowledge, is what I called feature construction. In this process, we should analyse our problem and convert it into mathematics problem, and come up with ideas what data we need and how to tackle this problem.</li></ul></li><li>Feature Extraction<ul><li>Extract the features from data set. Such as, in the document filtering or clustering task, to constuct the document/word vector, we use TF-IDF method to extract the information behind the documents. Another example in the CNN application, the kernels/filters in convolution layers are used to extract the features of images.</li></ul></li><li>Feature Selection<ul><li>Choose the most suitable features and feed them into our models. Ignore the non-relational features.</li></ul></li></ol><p>These three tasks sometimes will overlap and make people confused. They are basicall the good ways for me to understand, you can choose what your think to make yourself have a better understanding.</p><h2 id="How-to-do"><a href="#How-to-do" class="headerlink" title="How to do?"></a>How to do?</h2><p>A <strong>data science pipeline</strong> is basicall followed like this:</p><ol><li>given <strong>task</strong> and understand it</li><li>choose <strong>data set</strong></li><li><strong>pre-process</strong> the data set</li><li><strong>feature engineering</strong>(extract features)</li><li><strong>model</strong> data</li><li>analyse and evaluate</li></ol><p>Feature Engineering is a part of work in our data science project.</p><p>There are some ways to do features engineering:</p><ul><li>Brain storm: To come up the ideas of features which maybe useful for our project</li><li>Design features</li><li>Choose features</li><li></li></ul><p>(… TO BE CONTINUE)</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><ol><li>image and content of ideas from this <a href="http://www.csuldw.com/2015/10/24/2015-10-24%20feature%20engineering/" target="_blank" rel="noopener">blog</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Note-1: Linear Regression and Logistic Regression</title>
      <link href="/2019/04/12/Note-1-Linear-Regression-and-Logistic-Regression/"/>
      <url>/2019/04/12/Note-1-Linear-Regression-and-Logistic-Regression/</url>
      
        <content type="html"><![CDATA[<h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><h3 id="What’s-Liear-Regression"><a href="#What’s-Liear-Regression" class="headerlink" title="What’s Liear Regression"></a>What’s Liear Regression</h3><p>Linear Regression is a approach to modelling the relationship between a scalar response( or a <strong>dependent variable</strong>) and one or more explanatory variables. </p><p>It is written as the linear formula: $f(x)=w^T x+b$</p><p>Given the features values of $n$ data points, we can train to get a linear model which can fit the data set properly. When the new data point is fed into the model, we can predict the value.</p><p>We are going to find the optimal weights value:</p><script type="math/tex; mode=display">(w^*,b^*)= \underset{(w,b)}{\operatorname{argmix}}\sum^m_{i=1}(f(x_i)-y_i)^2</script><p>The close form solution can be calculated through the derivative. Of course, you can also use Gradient descent to find the optimal parameters, but it’s not necessary.</p><h3 id="Advantage"><a href="#Advantage" class="headerlink" title="Advantage"></a>Advantage</h3><p>The advantages of linear regression are that it’s simple and easy to implement, and the time complexity is small.</p><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><h3 id="Why-Logistic-Regression"><a href="#Why-Logistic-Regression" class="headerlink" title="Why Logistic Regression"></a>Why Logistic Regression</h3><p>Though the name of Logistic Regression includes <strong>regression</strong>, it’t not really a regression model. It’s for classification task. In this aspect, we can call <strong>Logistic Regression Analysis</strong>.</p><p>Since we have Linear Regression to do the <strong>regression</strong> task to predict the value for a new data set. Actually it can be used to predict the <strong>class</strong> for a given data. We can just set the threshold, if the predicted value is above the threshold, then it is classified into class 1, on the other hand, the data is classified into class 0.</p><p>However, there is a drawback when we use linear regression to do classification. We should set <strong>lots of thresholds</strong> according to different cases. And that’s why Logistic Regreesion came out.</p><h3 id="What’s-Logistic-Regression"><a href="#What’s-Logistic-Regression" class="headerlink" title="What’s Logistic Regression"></a>What’s Logistic Regression</h3><p><strong>Some key words in Logistic Regression</strong>:</p><ul><li>Hypothesis: Data points are Bernoulli distributed</li><li>Maximum likelihood to get the cost function </li><li>Gradient descent or Newton method to find the optimal solution</li></ul><p>Given the generalized linear model: $y=g^{-1}(w^Tx+b)$, the $g(\cdot)$ is called link function.</p><p>The $g$ function, from unit-stop function to sigmoid function, can convert the predicted value into corresponding class.</p><script type="math/tex; mode=display">sign(x)=\begin{cases}1,&x>0 \\ 0.5,&x=0 \cr 0,&x<0\end{cases}</script><p>Unit-step doesn’t have a very good property, it can easy to do derivativation. Then we use the sigmoid function. It has the format like this:</p><script type="math/tex; mode=display">y=\frac{1}{1+e^{-z}}</script><p><code>The sigmoid function squash the predicted value into 0 and 1, and now we can just set one threshold and do the classification task.</code></p><h3 id="Log-Odds-another-way-to-interprete-LR"><a href="#Log-Odds-another-way-to-interprete-LR" class="headerlink" title="Log Odds - another way to interprete LR"></a>Log Odds - another way to interprete LR</h3><p>Log odds is another way to interprete the logistic regression. For more details, see the chapter 3 in 《机器学习 周志华》.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>《机器学习》 周志华</li><li>Python数据科学 <a href="https://mp.weixin.qq.com/s/BJxdDz7DQg5QIWzzgNQrgA" target="_blank" rel="noopener">机器学习笔记</a></li><li><a href="https://www.zhihu.com/question/37031188" target="_blank" rel="noopener">最小二乘法的本质是什么</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dynamic Programming in Maximum Subarray</title>
      <link href="/2019/04/10/Dynamic-Programming-in-Maximum-Subarray/"/>
      <url>/2019/04/10/Dynamic-Programming-in-Maximum-Subarray/</url>
      
        <content type="html"><![CDATA[<h1 id="Dynamic-Programming-in-Maximum-Subarray"><a href="#Dynamic-Programming-in-Maximum-Subarray" class="headerlink" title="Dynamic Programming in Maximum Subarray"></a>Dynamic Programming in Maximum Subarray</h1><p>When I did the problem in Leetcode, the problem of <a href="https://leetcode.com/problems/maximum-subarray/" target="_blank" rel="noopener">53</a> is about the maximum subarray. I got in touch about the DP algorithm, which is very useful for solving this problem, converting the $O(n^3)$ to $O(n)$ complexity.</p><p>There are basically three approachs for this problem, the most time consuming one is the most straightforward and simple for user to come up with. Better one is the <code>DP algorithm</code>, which we are going to talk about.</p><h3 id="Problem-description"><a href="#Problem-description" class="headerlink" title="Problem description:"></a>Problem description:</h3><p>Given an integer array nums, find the contiguous subarray (containing at least one number) which has the largest sum and return its sum.</p><p><strong>Example</strong>:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: [-2,1,-3,4,-1,2,1,-5,4],</span><br><span class="line">Output: 6</span><br><span class="line">Explanation: [4,-1,2,1] has the largest sum = 6.</span><br></pre></td></tr></table></figure></p><p><strong>Answer</strong>:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def maxSubArray(self, nums: List[int]) -&gt; int:</span><br><span class="line">        <span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">        DP algorithm</span></span><br><span class="line"><span class="string">        '</span><span class="string">''</span></span><br><span class="line">        n = len(nums)</span><br><span class="line">        dp = [0]*(n)</span><br><span class="line">        dp[0] = nums[0]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(1,n):</span><br><span class="line">            dp[i] = max(dp[i-1] + nums[i], nums[i])</span><br><span class="line">        <span class="built_in">return</span> max(dp)</span><br></pre></td></tr></table></figure></p><p>See the video for more details:<br><div class="video-container"><iframe src="//www.youtube.com/embed/2MmGzdiKR9Y" frameborder="0" allowfullscreen></iframe></div></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><ol><li><a href="https://www.youtube.com/watch?v=2MmGzdiKR9Y" target="_blank" rel="noopener">Lecture</a> from Youtube. </li></ol>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Dynamic Programming </tag>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Computation Graph and Back Propagation</title>
      <link href="/2019/04/09/Computation-Graph-and-Back-Propagation/"/>
      <url>/2019/04/09/Computation-Graph-and-Back-Propagation/</url>
      
        <content type="html"><![CDATA[<h1 id="Computation-Graph-Back-Propagation-Forward-and-Reversed-Auto-Differentiation"><a href="#Computation-Graph-Back-Propagation-Forward-and-Reversed-Auto-Differentiation" class="headerlink" title="Computation Graph, Back Propagation, Forward and Reversed Auto Differentiation"></a>Computation Graph, Back Propagation, Forward and Reversed Auto Differentiation</h1><p>计算图，反向传播，前向和后向自动求导。</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Print in Python3</title>
      <link href="/2019/04/08/Print-in-Python3/"/>
      <url>/2019/04/08/Print-in-Python3/</url>
      
        <content type="html"><![CDATA[<h1 id="Print-in-Python3"><a href="#Print-in-Python3" class="headerlink" title="Print in Python3"></a>Print in Python3</h1><p>There are two kinds of ways to output to the screen, one is <code>sys.output.write</code> and <code>print</code>. When we use <code>print</code>, this built-in function actually calls the <code>stdout</code> function. </p><p><code>print</code> equals <code>stdout.write</code> plus <code>&quot;\n&quot;</code>. However, when we call <code>print</code>, we can change the optional parameter to print multi objects into a single line. </p><p>See the doc of <code>print</code>:</p><ul><li>print(*objects, sep=’ ‘, end=’\n’, file=sys.stdout, flush=False)</li></ul><p>To get the print without newline：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">"your output"</span>,end=<span class="string">" "</span>)</span><br></pre></td></tr></table></figure><hr><h4 id="My-use-experience"><a href="#My-use-experience" class="headerlink" title="My use experience:"></a>My use experience:</h4><p>See my exercise in the <a href="https://github.com/ShuoGH/datascience/blob/master/BinarySearchTree.ipynb" target="_blank" rel="noopener">binary search tree</a>, I want to travelsal the tree node and output into a single line.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">    def preOrderTraversal(self,node):</span><br><span class="line">        <span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">        if you want to output the node in a single line:</span></span><br><span class="line"><span class="string">            - change the `print` to `print(node.data,end='</span> <span class="string">')</span></span><br><span class="line"><span class="string">        '</span><span class="string">''</span></span><br><span class="line">        <span class="keyword">if</span> node:</span><br><span class="line"><span class="comment">#             print(node.data)</span></span><br><span class="line">            <span class="built_in">print</span>(node.data,end=<span class="string">" "</span>)</span><br><span class="line">            self.preOrderTraversal(node.lchild)</span><br><span class="line">            self.preOrderTraversal(node.rchild)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>List in Python</title>
      <link href="/2019/03/22/List-in-Python/"/>
      <url>/2019/03/22/List-in-Python/</url>
      
        <content type="html"><![CDATA[<h1 id="List-and-For-loop-in-Python"><a href="#List-and-For-loop-in-Python" class="headerlink" title="List and For loop in Python"></a>List and For loop in Python</h1><p>This blog also include some information of the array in Numpy.</p><p>Now have a list of <code>[1,-1,1,1,1,-1]</code>, want to have another list of category corresponding to the <code>1</code> and <code>-1</code>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; label=[1,-1,1,1,1,-1]</span><br><span class="line">&gt;&gt;&gt; cate =[<span class="string">'a'</span> <span class="keyword">if</span> i==1 <span class="keyword">else</span> <span class="string">'b'</span> <span class="keyword">for</span> i <span class="keyword">in</span> label]</span><br></pre></td></tr></table></figure><h2 id="set-value-using-the-filter-character-in-array-of-numpy"><a href="#set-value-using-the-filter-character-in-array-of-numpy" class="headerlink" title="set value using the filter character in array of numpy"></a>set value using the filter character in array of numpy</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; label=np.array([1,-1,1,1,1,-1]) <span class="comment"># it must be the array</span></span><br><span class="line">&gt;&gt;&gt; cate=np.empty(len(label))</span><br><span class="line">&gt;&gt;&gt; cate[label==1]=11</span><br><span class="line">&gt;&gt;&gt; cate[label==-1]=-11</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode Note Two Sum</title>
      <link href="/2019/03/21/LeetCode-Note-Two-Sum/"/>
      <url>/2019/03/21/LeetCode-Note-Two-Sum/</url>
      
        <content type="html"><![CDATA[<p>I have tried two kinds of the algorithm.</p><ol><li><p>Do sum by loop and check whether the sum is equal to the target</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">    sumList = [nums[i]+item <span class="keyword">for</span> item <span class="keyword">in</span> nums[i+1:]]</span><br><span class="line">    <span class="keyword">for</span> sumNum <span class="keyword">in</span> sumList:</span><br><span class="line">        <span class="keyword">if</span> sumNum == target:</span><br><span class="line">            <span class="built_in">return</span> [i, sumList.index(sumNum)+i+1]</span><br></pre></td></tr></table></figure></li><li><p>Use the <code>index()</code> in List<br>By using this, the performance become much better.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">    targetNum = target-nums[i]</span><br><span class="line">    <span class="keyword">if</span> targetNum <span class="keyword">in</span> nums and nums.index(targetNum) != i:</span><br><span class="line">        <span class="built_in">return</span> i, nums.index(targetNum)</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ensemble Learning</title>
      <link href="/2019/03/20/Ensemble-Learning/"/>
      <url>/2019/03/20/Ensemble-Learning/</url>
      
        <content type="html"><![CDATA[<h1 id="Ensemble-Learning"><a href="#Ensemble-Learning" class="headerlink" title="Ensemble Learning"></a>Ensemble Learning</h1><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>When to use bagging?</p><p>用于很强的model。</p><p>最容易overfitting的model其实不是神经网络，而是decision tree。如果你想，只要把树扎的足够深，甚至可以在training data上得到100%的准确率，但是那没有任何意义，只是overfitting而已。</p><p>Bagging就是将容易overfitting的一堆model结合起来，乱拳打死老师傅。随机森林就是在decision tree上进行bagging，将多个决策树组合起来组成随机森林。</p><p>How to get different classifier?</p><ul><li>Re-sampling your data set to form a new set </li><li>Re-weighting yoru data set to form a new set</li></ul><h3 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h3><p>The data set is generated by the bootstrapping, which resample the data set with replacement. In random forest, we average much less correlated trees. To implement this algorithm, not only different data subsets are used, but also we choose a subset $m \ll p$ of the features to train decision tree. Typically $m$ can range from $1$ to $\sqrt{p}$. The trees are not that good, but by averaging over huge number of trees, we can get pretty good results.</p><h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>用于比较弱的model。</p><p>Adaboost</p><p>Can convert the weak learner to strong learner(classifier).</p><p>我自己的一个简单Adaboost <a href="https://github.com/ShuoGH/datascience/blob/master/Adaboost.ipynb" target="_blank" rel="noopener">demo</a></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>台湾大学李宏毅的<a href="https://www.youtube.com/watch?v=tH9FH1DH5n0" target="_blank" rel="noopener">视频</a></li><li>课程资源: <a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html" target="_blank" rel="noopener">Hung-yi Lee</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux Use</title>
      <link href="/2019/03/20/Linux-Use/"/>
      <url>/2019/03/20/Linux-Use/</url>
      
        <content type="html"><![CDATA[<p>This blog records some basic commends about the use in Linux Server.</p><h1 id="SSH"><a href="#SSH" class="headerlink" title="SSH"></a>SSH</h1><p>SSH is used to make connection with the remote server.</p><h2 id="Connect-the-server"><a href="#Connect-the-server" class="headerlink" title="Connect the server"></a>Connect the server</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh xxx@xxx.ecs.soton.ac.uk</span><br></pre></td></tr></table></figure><h2 id="Close-the-connection"><a href="#Close-the-connection" class="headerlink" title="Close the connection"></a>Close the connection</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure><h1 id="SCP"><a href="#SCP" class="headerlink" title="SCP"></a>SCP</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SCP is <span class="keyword">for</span> transport between the <span class="built_in">local</span> and remote.</span><br></pre></td></tr></table></figure><h2 id="Transfer-the-entire-file-folder"><a href="#Transfer-the-entire-file-folder" class="headerlink" title="Transfer the entire file folder"></a>Transfer the entire file folder</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The formmer path is the `from` location, the latter one is the destination.</span><br><span class="line">scp -r xxx@xxx.ecs.soton.ac.uk:/home/sa2y18/mydocuments/4_dataMining ~/Desktop/</span><br></pre></td></tr></table></figure><h2 id="Transfer-a-single-file"><a href="#Transfer-a-single-file" class="headerlink" title="Transfer a single file"></a>Transfer a single file</h2><p>no <code>-r</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp xxx@xxx.ecs.soton.ac.uk:PATH LOCALPATH</span><br></pre></td></tr></table></figure></p><hr><h1 id="Use-Jupyter"><a href="#Use-Jupyter" class="headerlink" title="Use Jupyter"></a>Use Jupyter</h1><p>Run jupyter on the remote and open it from local port. You can follow the instruction: [<a href="http://fizzylogic.nl/2017/11/06/edit-jupyter-notebooks-over-ssh/" target="_blank" rel="noopener">http://fizzylogic.nl/2017/11/06/edit-jupyter-notebooks-over-ssh/</a>]<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --no-browser --port=8080  <span class="comment"># run on the remote machine</span></span><br><span class="line"></span><br><span class="line">ssh -N -L 8080:localhost:8080 xxx@xxx.ecs.soton.ac.uk  <span class="comment"># run in the local machine terminal</span></span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Beyond Accuracy: Precision and Recall</title>
      <link href="/2019/03/12/Beyond-Accuracy-Precision-and-Recall/"/>
      <url>/2019/03/12/Beyond-Accuracy-Precision-and-Recall/</url>
      
        <content type="html"><![CDATA[<p>After training a model, there are some metrics to measure the performance of the model. The <code>accuracy</code> is the common one. Besides, there are other things to measure the performance.</p><p><strong>Given four cases of the results</strong>:<br>True positive (TP): actually positive, predictive is positive which is true<br>False positive (FP): actually negative, predictive is positive which is false (type 1 error)<br>True negative (TN): actually negative, predictive is negative which is true<br>False negative (FN): actually positive, predictive is negative which is false (type 2 error)</p><div class="table-container"><table><thead><tr><th style="text-align:left"></th><th style="text-align:right">True</th><th style="text-align:center">False</th></tr></thead><tbody><tr><td style="text-align:left">Positve</td><td style="text-align:right">TP</td><td style="text-align:center">FP</td></tr><tr><td style="text-align:left">Negative</td><td style="text-align:right">TN</td><td style="text-align:center">FN</td></tr></tbody></table></div><p>$ Precision = \frac{TP}{TP+FP} $<br>$ Recall = \frac{TP}{TP+FN} $</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Performance Measurement </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo writing</title>
      <link href="/2019/03/11/Hexo-writing/"/>
      <url>/2019/03/11/Hexo-writing/</url>
      
        <content type="html"><![CDATA[<p>Sometimes you don’t want to see the blog in the first time as you haven’t finished it.<br>In this case, you can use the <code>draft</code>.</p><h1 id="Initialize-draft-and-publish"><a href="#Initialize-draft-and-publish" class="headerlink" title="Initialize draft and publish"></a>Initialize draft and publish</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new draft <span class="string">"draft name"</span></span><br><span class="line">$ <span class="comment">#before publishing, recommend to use Hexo clean</span></span><br><span class="line">$ <span class="comment"># hexo clean</span></span><br><span class="line">$ hexo publish <span class="string">"draft name"</span></span><br></pre></td></tr></table></figure><h1 id="Other-use-skills"><a href="#Other-use-skills" class="headerlink" title="Other use skills"></a>Other use skills</h1><p><strong> Add the link</strong><br><code>[words](link url)</code></p>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git add ignore</title>
      <link href="/2019/03/11/Git-add-ignore/"/>
      <url>/2019/03/11/Git-add-ignore/</url>
      
        <content type="html"><![CDATA[<h1 id="What’s-gitignore-for"><a href="#What’s-gitignore-for" class="headerlink" title="What’s gitignore for:"></a>What’s gitignore for:</h1><p>The files which you don’t want to upload or list in your git history, such as “.DS_store” or some other data files. You can achieve this by adding the <code>.gitignore</code> file into your git repo. </p><h1 id="Mac-DS-store"><a href="#Mac-DS-store" class="headerlink" title="Mac .DS_store"></a>Mac .DS_store</h1><p>gitignore can set globally or just add a single file into your repo. You can find the doc in this <a href="https://git-scm.com/docs/gitignore" target="_blank" rel="noopener">link</a>.</p><ul><li>所有空行或者以注释符号 ＃ 开头的行都会被 git 忽略，空行可以为了可读性分隔段落，# 表明注释。</li><li>第一个 / 会匹配路径的根目录，举个栗子，”/*.html”会匹配”index.html”，而不是”d/index.html”。</li><li>通配符 <em> 匹配任意个任意字符，? 匹配一个任意字符。需要注意的是通配符不会匹配文件路径中的 /，举个栗子，”d/</em>.html”会匹配”d/index.html”，但不会匹配”d/a/b/c/index.html”。</li><li>两个连续的星号 ** 有特殊含义：<ul><li>以 <strong>/ 开头表示匹配所有的文件夹，例如 </strong>/test.md 匹配所有的test.md文件。</li><li>以 /<strong> 结尾表示匹配文件夹内所有内容，例如 a/</strong> 匹配文件夹a中所有内容。</li><li>连续星号 <strong> 前后分别被 / 夹住表示匹配0或者多层文件夹，例如 a/</strong>/b 匹配到 a/b 、a/x/b 、a/x/y/b 等。</li></ul></li><li>前缀 ! 的模式表示如果前面匹配到被忽略，则重新添加回来。如果匹配到的父文件夹还是忽略状态，该文件还是保持忽略状态。如果路径名第一个字符为 ! ，则需要在前面增加 \ 进行转义。</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://git-scm.com/docs/gitignore" target="_blank" rel="noopener">git doc</a></li><li><a href="https://orianna-zzo.github.io/sci-tech/2018-01/mac%E4%B8%ADgit%E5%BF%BD%E7%95%A5.ds_store%E6%96%87%E4%BB%B6/" target="_blank" rel="noopener">Mac中Git忽略.DS_Store文件</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git Use</title>
      <link href="/2019/03/10/Git-Use/"/>
      <url>/2019/03/10/Git-Use/</url>
      
        <content type="html"><![CDATA[<p>Quick guide book:</p><h1 id="Clone-repo"><a href="#Clone-repo" class="headerlink" title="Clone repo"></a>Clone repo</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span></span><br></pre></td></tr></table></figure><h1 id="Initialize-new-repo"><a href="#Initialize-new-repo" class="headerlink" title="Initialize new repo"></a>Initialize new repo</h1><p> Initialize and push it to a new repo in Github<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git init</span><br><span class="line">$ git remote add origin https://github.com/ShuoGH/REPONAME.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># add the repo in the github website in advance</span></span><br><span class="line">$ git push -u origin master   <span class="comment"># first to push </span></span><br><span class="line">$ git push origin master  <span class="comment">#after the first time</span></span><br></pre></td></tr></table></figure></p><h1 id="Branch"><a href="#Branch" class="headerlink" title="Branch"></a>Branch</h1><p>Some commend about the branches:</p><h2 id="List-branches"><a href="#List-branches" class="headerlink" title="List branches:"></a>List branches:</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -a</span><br></pre></td></tr></table></figure><h2 id="Check-out-branch"><a href="#Check-out-branch" class="headerlink" title="Check out branch"></a>Check out branch</h2><p>If you want to checkout to a branch directly, just use:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout origin/xxbranch</span><br></pre></td></tr></table></figure></p><p>To create a local branch,<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b xx origin/xxbranch</span><br><span class="line"><span class="comment"># or use </span></span><br><span class="line">git checkout -t origin/xxbranch` <span class="comment"># it will create a branch with same name.</span></span><br><span class="line"><span class="comment"># or use `fetch`</span></span><br><span class="line">git fetch origin xxbranch:xx <span class="comment"># (in this way, it won't checkout to the new branch automatically)</span></span><br></pre></td></tr></table></figure></p><h2 id="Delete-branch"><a href="#Delete-branch" class="headerlink" title="Delete branch"></a>Delete branch</h2><p>To delete the remote branch:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push &lt;remote_name&gt; --delete &lt;branch_name&gt;</span><br></pre></td></tr></table></figure></p><h1 id="Operation-on-Files"><a href="#Operation-on-Files" class="headerlink" title="Operation on Files"></a>Operation on Files</h1><p>To rm the local file and make the change into git:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git rm xx </span><br><span class="line">git commit -m <span class="string">"xxxx"</span></span><br></pre></td></tr></table></figure></p><h1 id="Show-the-git-tree"><a href="#Show-the-git-tree" class="headerlink" title="Show the git tree"></a>Show the git tree</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --graph --pretty=oneline --abbrev-commit</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/03/10/hello-world/"/>
      <url>/2019/03/10/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>If you want to write a draft </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new draft <span class="string">"My New Draft"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br><span class="line">or</span><br><span class="line">$ hexo s</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br><span class="line">or </span><br><span class="line">$ hexo g</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br><span class="line">or </span><br><span class="line">$ hexo d</span><br></pre></td></tr></table></figure><p>Before deployment, you’d better use<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br></pre></td></tr></table></figure></p><h3 id="Change-the-display-in-Home-page"><a href="#Change-the-display-in-Home-page" class="headerlink" title="Change the display in Home page"></a>Change the display in Home page</h3><p>If you don’t want to show the full article, it can be realised by modifying the setting<br><code>auto_excerpt</code> in the <code>_config.yml</code> file.</p><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
